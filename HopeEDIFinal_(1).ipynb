{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HopeEDIFinal (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu5ZtPEjvq1n",
        "outputId": "bfbaaa08-7c58-4e7f-f02a-3ab815e8b9ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8POSCiHv1km",
        "outputId": "b26bfda7-295b-40e9-98d2-d821030981ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb  3 17:25:31 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries needed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "metadata": {
        "id": "wuFTPaHX2qtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv('/content/drive/MyDrive/HopeCodalab/Hope_ENG_train.csv',names=['text','label'])\n",
        "test=pd.read_csv('/content/drive/MyDrive/HopeCodalab/Hope_ENG_dev.csv',names=['text','label'])\n",
        "eval=pd.read_csv('/content/drive/MyDrive/HopeCodalab/Hope_ENG_test.csv',names=['text'])\n",
        "neweval=pd.read_csv('/content/drive/MyDrive/HopeCodalab/newTest.csv',names=['text'])"
      ],
      "metadata": {
        "id": "2VVtKwqiv3PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "L8zr6jLkUYBC",
        "outputId": "7b8c087b-bbef-4c9d-85aa-22935e93cdf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f08d7178-108c-401f-8b9f-d773f23fe4f1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>these tiktoks radiate gay chaotic energy and i...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@Champions Again He got killed for using false...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It's not that all lives don't matter</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Is it really that difficult to understand? Bla...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Whenever we say black isn't that racists?  Why...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22735</th>\n",
              "      <td>It's a load of bollocks every life matters sim...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22736</th>\n",
              "      <td>no say it because all lives matter! deku would...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22737</th>\n",
              "      <td>God says her life matters</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22738</th>\n",
              "      <td>This video is just shit. A bunch of whiny ass ...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22739</th>\n",
              "      <td>Mc Fortnut2821 she did 4 months ago in west ch...</td>\n",
              "      <td>Non_hope_speech</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22740 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f08d7178-108c-401f-8b9f-d773f23fe4f1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f08d7178-108c-401f-8b9f-d773f23fe4f1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f08d7178-108c-401f-8b9f-d773f23fe4f1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    text            label\n",
              "0      these tiktoks radiate gay chaotic energy and i...  Non_hope_speech\n",
              "1      @Champions Again He got killed for using false...  Non_hope_speech\n",
              "2                   It's not that all lives don't matter  Non_hope_speech\n",
              "3      Is it really that difficult to understand? Bla...  Non_hope_speech\n",
              "4      Whenever we say black isn't that racists?  Why...  Non_hope_speech\n",
              "...                                                  ...              ...\n",
              "22735  It's a load of bollocks every life matters sim...  Non_hope_speech\n",
              "22736  no say it because all lives matter! deku would...  Non_hope_speech\n",
              "22737                          God says her life matters  Non_hope_speech\n",
              "22738  This video is just shit. A bunch of whiny ass ...  Non_hope_speech\n",
              "22739  Mc Fortnut2821 she did 4 months ago in west ch...  Non_hope_speech\n",
              "\n",
              "[22740 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def simple_text_clean(x):\n",
        "    # first we lowercase everything\n",
        "    x = x.lower()\n",
        "    # remove unicode characters\n",
        "    x = x.encode('ascii', 'ignore').decode()\n",
        "    #remove websites\n",
        "    x = re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil)[\\S]*\\s?','',x)\n",
        "    x = re.sub(r'[^a-zA-z.,!?/;\\\"\\'\\s]','',x)#removes special characters except some & remove nums 0-9\n",
        "    return x"
      ],
      "metadata": {
        "id": "3bMo0UAvw_4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def changelabel_cleantext(temp):\n",
        "  temp2=temp.label\n",
        "  res=pd.DataFrame(temp2)\n",
        "  for i in res:\n",
        "      res.loc[res[str(f'{i}')]== 'Non_hope_speech', str(f'{i}')] = 0\n",
        "      res.loc[res[str(f'{i}')]== 'Hope_speech', str(f'{i}')] = 1\n",
        "      res[str(f'{i}')]=res[str(f'{i}')].astype(str).astype(int)\n",
        "  temp['text']=temp['text'].apply(str).apply(lambda x: simple_text_clean(x))\n",
        "  newdf=pd.concat([temp['text'],res],axis=1)\n",
        "  return newdf"
      ],
      "metadata": {
        "id": "W_xYN2rM1acB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=changelabel_cleantext(train)\n",
        "test=changelabel_cleantext(test)"
      ],
      "metadata": {
        "id": "UVCMQeaK1dEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large', truncation=True, do_lower_case=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZa5q-e61hWa",
        "outputId": "d34f16ed-c707-4d6c-8b02-eeda4a956d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import statistics as st\n",
        "# max_len = 0\n",
        "# lens=[]\n",
        "# sentences=train.text\n",
        "\n",
        "# # For every sentence...\n",
        "# for sent in sentences:\n",
        "\n",
        "#     # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "#     input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "#     lens.append(len(input_ids))\n",
        "#     # Update the maximum sentence length.\n",
        "#     max_len = max(max_len, len(input_ids))\n",
        "\n",
        "# print('Max sentence length: ', max_len)\n",
        "# print('Max sentence length: ', st.mean(lens))"
      ],
      "metadata": {
        "id": "qw43JRUi1v3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = self.data.text\n",
        "        self.targets = self.data.label\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "azD0cObP2Xiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = RobertaData(train, tokenizer, 22)\n",
        "testing_set = RobertaData(test, tokenizer, 22)"
      ],
      "metadata": {
        "id": "SYnhmBby21QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': 16,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': 16,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "POKUlWuN3oy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "robertamodel = RobertaModel.from_pretrained('roberta-large',output_hidden_states=True)\n",
        "# for p in robertamodel.parameters():\n",
        "#     p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrlAbcc34zRo",
        "outputId": "a379b45e-1b68-4349-a353-34f34bb01043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        self.l1 = robertamodel\n",
        "        self.pre_classifier = torch.nn.Linear(4096, 4096)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.classifier = torch.nn.Linear(4096, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_states = output_1[-1]\n",
        "        big = torch.cat(tuple([hidden_states[i] for i in [-4, -3, -2, -1]]), dim=-1)\n",
        "        big = big[:, 0,:]\n",
        "        pooler = self.pre_classifier(big)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "metadata": {
        "id": "GDtEnlQn302J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "model = RobertaClass()\n",
        "model.to(device)\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzSg2KeY36D3",
        "outputId": "5d63bff4-195b-4764-8f98-1cc1e7a0a78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaClass(\n",
              "  (l1): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (12): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (13): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (14): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (15): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (16): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (17): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (18): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (19): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (20): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (21): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (22): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (23): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=4096, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the loss function and optimizer\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "UpQ4j3-C4DSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ],
      "metadata": {
        "id": "k-7pVT2Q5LNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm as tqdm"
      ],
      "metadata": {
        "id": "oAm94kjuC53N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score"
      ],
      "metadata": {
        "id": "-13ZliubFc_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm, trange"
      ],
      "metadata": {
        "id": "zXpXR3XzFmdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model,a, tokenizer, criterion, dataloader, tres = 0.5): \n",
        "    \n",
        "    # Eval!\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = None\n",
        "    proba = None\n",
        "    out_label_ids = None\n",
        "    for batch in a(dataloader):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            ids = batch['ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = batch['targets']\n",
        "            targets=targets.view(targets.size(0),-1)\n",
        "            labels=targets.to(device, dtype = torch.float32)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            logits = outputs  # model outputs are always tuple in transformers (see doc)\n",
        "            tmp_eval_loss = criterion(logits, labels) \n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "        if preds is None:\n",
        "            preds = torch.sigmoid(logits).detach().cpu().numpy() > tres\n",
        "            proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            out_label_ids = labels.detach().cpu().numpy()\n",
        "        else:            \n",
        "            preds = np.append(preds, torch.sigmoid(logits).detach().cpu().numpy() > tres, axis=0)\n",
        "            proba = np.append(proba, torch.sigmoid(logits).detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, labels.detach().cpu().numpy(), axis=0)\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "\n",
        "    result = {\n",
        "        \"loss\": eval_loss,\n",
        "        \"accuracy\": accuracy_score(out_label_ids, preds),\n",
        "        \"AUC\": roc_auc_score(out_label_ids, proba),\n",
        "        \"micro_f1\": f1_score(out_label_ids, preds, average=\"micro\"),\n",
        "        \"prediction\": preds,\n",
        "        \"labels\": out_label_ids,\n",
        "        \"proba\": proba\n",
        "    }\n",
        "    \n",
        "    return result"
      ],
      "metadata": {
        "id": "7DUVu-cdFJ8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(save_path, model, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "    \n",
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "_kb4Q4NEGe8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_epochs=3"
      ],
      "metadata": {
        "id": "IXYNOmG8HNBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_step = 0\n",
        "gradient_accumulation_steps=20\n",
        "max_grad_norm=1\n",
        "global_step = 0\n",
        "train_step = 0\n",
        "tr_loss, logging_loss = 0.0, 0.0\n",
        "best_valid_f1 = 0.73\n",
        "global_steps_list = []\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "val_acc_list = []\n",
        "val_auc_list = []\n",
        "eval_every = len(training_loader) // 7\n",
        "running_loss = 0\n",
        "file_path='/content/drive/MyDrive/RajWorkspace/mytestresults'\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "for i in range(num_train_epochs):\n",
        "    print(\"Epoch\", i+1, f\"from {num_train_epochs}\")\n",
        "    whole_y_pred=np.array([])\n",
        "    whole_y_t=np.array([])\n",
        "    for step, batch in enumerate(tqdm(training_loader)):\n",
        "        model.train()\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets']\n",
        "        targets=targets.view(targets.size(0),-1)\n",
        "        labels=targets.to(device, dtype = torch.float32)\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        logits = outputs   # model outputs are always tuple in transformers (see doc)\n",
        "        loss = criterion(logits, labels) \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "        if (step + 1) % eval_every == 0:\n",
        "            \n",
        "            val_result = evaluate(model,tqdm, tokenizer, criterion, testing_loader)\n",
        "            \n",
        "            val_loss_list.append(val_result['loss'])\n",
        "            val_acc_list.append(val_result['accuracy'])\n",
        "            val_auc_list.append(val_result['AUC'])\n",
        "            val_auc_list.append(val_result['micro_f1'])\n",
        "            \n",
        "            # checkpoint\n",
        "            if val_result['micro_f1'] > best_valid_f1:\n",
        "                best_valid_auc = val_result['AUC']\n",
        "                val_loss = val_result['loss']\n",
        "                val_acc = val_result['accuracy']\n",
        "                best_valid_f1=val_result['micro_f1']\n",
        "                model_path = f'{file_path}/model-auc{best_valid_auc:.3f}-loss{val_loss:.3f}-acc{val_acc:.3f}.pt'\n",
        "                print(f\"AUC improved, so saving this model\")  \n",
        "                save_checkpoint(model_path, model, val_result['loss'])              \n",
        "            \n",
        "            print(  \"Val loss:\", f\"{val_result['loss']:.4f}\",\n",
        "                    \"Val acc:\", f\"{val_result['accuracy']:.4f}\",\n",
        "                    \"Val f1:\", f\"{val_result['micro_f1']:.4f}\",\n",
        "                    \"AUC:\", f\"{val_result['AUC']:.4f}\")   \n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7JZs9m4Q5YWq",
        "outputId": "1bdca5a0-668e-4c7d-a890-48a3a020d564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 from 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1422 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            " 14%|█▍        | 202/1422 [00:40<04:05,  4.97it/s]\n",
            "  0%|          | 0/178 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 2/178 [00:00<00:09, 17.79it/s]\u001b[A\n",
            "  2%|▏         | 4/178 [00:00<00:09, 17.93it/s]\u001b[A\n",
            "  3%|▎         | 6/178 [00:00<00:09, 17.83it/s]\u001b[A\n",
            "  4%|▍         | 8/178 [00:00<00:09, 17.94it/s]\u001b[A\n",
            "  6%|▌         | 10/178 [00:00<00:09, 18.12it/s]\u001b[A\n",
            "  7%|▋         | 12/178 [00:00<00:09, 18.18it/s]\u001b[A\n",
            "  8%|▊         | 14/178 [00:00<00:09, 18.19it/s]\u001b[A\n",
            "  9%|▉         | 16/178 [00:00<00:08, 18.08it/s]\u001b[A\n",
            " 10%|█         | 18/178 [00:00<00:08, 18.00it/s]\u001b[A\n",
            " 11%|█         | 20/178 [00:01<00:08, 17.98it/s]\u001b[A\n",
            " 12%|█▏        | 22/178 [00:01<00:08, 18.15it/s]\u001b[A\n",
            " 13%|█▎        | 24/178 [00:01<00:08, 18.16it/s]\u001b[A\n",
            " 15%|█▍        | 26/178 [00:01<00:08, 18.10it/s]\u001b[A\n",
            " 16%|█▌        | 28/178 [00:01<00:08, 18.11it/s]\u001b[A\n",
            " 17%|█▋        | 30/178 [00:01<00:08, 18.16it/s]\u001b[A\n",
            " 18%|█▊        | 32/178 [00:01<00:08, 17.91it/s]\u001b[A\n",
            " 19%|█▉        | 34/178 [00:01<00:08, 17.97it/s]\u001b[A\n",
            " 20%|██        | 36/178 [00:01<00:07, 18.10it/s]\u001b[A\n",
            " 21%|██▏       | 38/178 [00:02<00:07, 18.17it/s]\u001b[A\n",
            " 22%|██▏       | 40/178 [00:02<00:07, 18.18it/s]\u001b[A\n",
            " 24%|██▎       | 42/178 [00:02<00:07, 18.19it/s]\u001b[A\n",
            " 25%|██▍       | 44/178 [00:02<00:07, 18.23it/s]\u001b[A\n",
            " 26%|██▌       | 46/178 [00:02<00:07, 18.25it/s]\u001b[A\n",
            " 27%|██▋       | 48/178 [00:02<00:07, 18.38it/s]\u001b[A\n",
            " 28%|██▊       | 50/178 [00:02<00:06, 18.32it/s]\u001b[A\n",
            " 29%|██▉       | 52/178 [00:02<00:06, 18.22it/s]\u001b[A\n",
            " 30%|███       | 54/178 [00:02<00:06, 18.21it/s]\u001b[A\n",
            " 31%|███▏      | 56/178 [00:03<00:06, 18.15it/s]\u001b[A\n",
            " 33%|███▎      | 58/178 [00:03<00:06, 18.26it/s]\u001b[A\n",
            " 34%|███▎      | 60/178 [00:03<00:06, 18.12it/s]\u001b[A\n",
            " 35%|███▍      | 62/178 [00:03<00:06, 18.22it/s]\u001b[A\n",
            " 36%|███▌      | 64/178 [00:03<00:06, 18.21it/s]\u001b[A\n",
            " 37%|███▋      | 66/178 [00:03<00:06, 18.08it/s]\u001b[A\n",
            " 38%|███▊      | 68/178 [00:03<00:06, 18.24it/s]\u001b[A\n",
            " 39%|███▉      | 70/178 [00:03<00:05, 18.17it/s]\u001b[A\n",
            " 40%|████      | 72/178 [00:03<00:05, 18.18it/s]\u001b[A\n",
            " 42%|████▏     | 74/178 [00:04<00:05, 18.27it/s]\u001b[A\n",
            " 43%|████▎     | 76/178 [00:04<00:05, 18.15it/s]\u001b[A\n",
            " 44%|████▍     | 78/178 [00:04<00:05, 18.18it/s]\u001b[A\n",
            " 45%|████▍     | 80/178 [00:04<00:05, 18.04it/s]\u001b[A\n",
            " 46%|████▌     | 82/178 [00:04<00:05, 18.15it/s]\u001b[A\n",
            " 47%|████▋     | 84/178 [00:04<00:05, 18.19it/s]\u001b[A\n",
            " 48%|████▊     | 86/178 [00:04<00:05, 18.06it/s]\u001b[A\n",
            " 49%|████▉     | 88/178 [00:04<00:04, 18.10it/s]\u001b[A\n",
            " 51%|█████     | 90/178 [00:04<00:04, 18.20it/s]\u001b[A\n",
            " 52%|█████▏    | 92/178 [00:05<00:04, 18.29it/s]\u001b[A\n",
            " 53%|█████▎    | 94/178 [00:05<00:04, 18.18it/s]\u001b[A\n",
            " 54%|█████▍    | 96/178 [00:05<00:04, 18.21it/s]\u001b[A\n",
            " 55%|█████▌    | 98/178 [00:05<00:04, 18.20it/s]\u001b[A\n",
            " 56%|█████▌    | 100/178 [00:05<00:04, 18.01it/s]\u001b[A\n",
            " 57%|█████▋    | 102/178 [00:05<00:04, 18.06it/s]\u001b[A\n",
            " 58%|█████▊    | 104/178 [00:05<00:04, 18.17it/s]\u001b[A\n",
            " 60%|█████▉    | 106/178 [00:05<00:03, 18.04it/s]\u001b[A\n",
            " 61%|██████    | 108/178 [00:05<00:03, 18.06it/s]\u001b[A\n",
            " 62%|██████▏   | 110/178 [00:06<00:03, 18.08it/s]\u001b[A\n",
            " 63%|██████▎   | 112/178 [00:06<00:03, 18.07it/s]\u001b[A\n",
            " 64%|██████▍   | 114/178 [00:06<00:03, 18.09it/s]\u001b[A\n",
            " 65%|██████▌   | 116/178 [00:06<00:03, 18.16it/s]\u001b[A\n",
            " 66%|██████▋   | 118/178 [00:06<00:03, 18.18it/s]\u001b[A\n",
            " 67%|██████▋   | 120/178 [00:06<00:03, 18.20it/s]\u001b[A\n",
            " 69%|██████▊   | 122/178 [00:06<00:03, 18.23it/s]\u001b[A\n",
            " 70%|██████▉   | 124/178 [00:06<00:02, 18.28it/s]\u001b[A\n",
            " 71%|███████   | 126/178 [00:06<00:02, 18.25it/s]\u001b[A\n",
            " 72%|███████▏  | 128/178 [00:07<00:02, 18.34it/s]\u001b[A\n",
            " 73%|███████▎  | 130/178 [00:07<00:02, 18.38it/s]\u001b[A\n",
            " 74%|███████▍  | 132/178 [00:07<00:02, 18.43it/s]\u001b[A\n",
            " 75%|███████▌  | 134/178 [00:07<00:02, 18.37it/s]\u001b[A\n",
            " 76%|███████▋  | 136/178 [00:07<00:02, 18.32it/s]\u001b[A\n",
            " 78%|███████▊  | 138/178 [00:07<00:02, 18.23it/s]\u001b[A\n",
            " 79%|███████▊  | 140/178 [00:07<00:02, 18.17it/s]\u001b[A\n",
            " 80%|███████▉  | 142/178 [00:07<00:01, 18.25it/s]\u001b[A\n",
            " 81%|████████  | 144/178 [00:07<00:01, 18.29it/s]\u001b[A\n",
            " 82%|████████▏ | 146/178 [00:08<00:01, 18.25it/s]\u001b[A\n",
            " 83%|████████▎ | 148/178 [00:08<00:01, 18.25it/s]\u001b[A\n",
            " 84%|████████▍ | 150/178 [00:08<00:01, 18.11it/s]\u001b[A\n",
            " 85%|████████▌ | 152/178 [00:08<00:01, 18.19it/s]\u001b[A\n",
            " 87%|████████▋ | 154/178 [00:08<00:01, 18.18it/s]\u001b[A\n",
            " 88%|████████▊ | 156/178 [00:08<00:01, 18.25it/s]\u001b[A\n",
            " 89%|████████▉ | 158/178 [00:08<00:01, 18.14it/s]\u001b[A\n",
            " 90%|████████▉ | 160/178 [00:08<00:00, 18.10it/s]\u001b[A\n",
            " 91%|█████████ | 162/178 [00:08<00:00, 18.10it/s]\u001b[A\n",
            " 92%|█████████▏| 164/178 [00:09<00:00, 18.24it/s]\u001b[A\n",
            " 93%|█████████▎| 166/178 [00:09<00:00, 18.19it/s]\u001b[A\n",
            " 94%|█████████▍| 168/178 [00:09<00:00, 18.16it/s]\u001b[A\n",
            " 96%|█████████▌| 170/178 [00:09<00:00, 18.18it/s]\u001b[A\n",
            " 97%|█████████▋| 172/178 [00:09<00:00, 18.16it/s]\u001b[A\n",
            " 98%|█████████▊| 174/178 [00:09<00:00, 18.21it/s]\u001b[A\n",
            "100%|██████████| 178/178 [00:09<00:00, 18.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC improved, so saving this model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 203/1422 [00:55<1:33:39,  4.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to ==> /content/drive/MyDrive/RajWorkspace/mytestresults/model-auc0.501-loss0.316-acc0.904.pt\n",
            "Val loss: 0.3164 Val acc: 0.9043 Val f1: 0.9043 AUC: 0.5011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            " 28%|██▊       | 405/1422 [01:36<03:23,  5.00it/s]\n",
            "  0%|          | 0/178 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 2/178 [00:00<00:09, 18.40it/s]\u001b[A\n",
            "  2%|▏         | 4/178 [00:00<00:09, 18.54it/s]\u001b[A\n",
            "  3%|▎         | 6/178 [00:00<00:09, 18.66it/s]\u001b[A\n",
            "  4%|▍         | 8/178 [00:00<00:09, 18.38it/s]\u001b[A\n",
            "  6%|▌         | 10/178 [00:00<00:09, 18.43it/s]\u001b[A\n",
            "  7%|▋         | 12/178 [00:00<00:09, 18.36it/s]\u001b[A\n",
            "  8%|▊         | 14/178 [00:00<00:08, 18.31it/s]\u001b[A\n",
            "  9%|▉         | 16/178 [00:00<00:08, 18.32it/s]\u001b[A\n",
            " 10%|█         | 18/178 [00:00<00:08, 18.36it/s]\u001b[A\n",
            " 11%|█         | 20/178 [00:01<00:08, 18.38it/s]\u001b[A\n",
            " 12%|█▏        | 22/178 [00:01<00:08, 18.32it/s]\u001b[A\n",
            " 13%|█▎        | 24/178 [00:01<00:08, 18.36it/s]\u001b[A\n",
            " 15%|█▍        | 26/178 [00:01<00:08, 18.44it/s]\u001b[A\n",
            " 16%|█▌        | 28/178 [00:01<00:08, 18.46it/s]\u001b[A\n",
            " 17%|█▋        | 30/178 [00:01<00:08, 18.48it/s]\u001b[A\n",
            " 18%|█▊        | 32/178 [00:01<00:07, 18.46it/s]\u001b[A\n",
            " 19%|█▉        | 34/178 [00:01<00:07, 18.51it/s]\u001b[A\n",
            " 20%|██        | 36/178 [00:01<00:07, 18.51it/s]\u001b[A\n",
            " 21%|██▏       | 38/178 [00:02<00:07, 18.57it/s]\u001b[A\n",
            " 22%|██▏       | 40/178 [00:02<00:07, 18.44it/s]\u001b[A\n",
            " 24%|██▎       | 42/178 [00:02<00:07, 18.47it/s]\u001b[A\n",
            " 25%|██▍       | 44/178 [00:02<00:07, 18.46it/s]\u001b[A\n",
            " 26%|██▌       | 46/178 [00:02<00:07, 18.40it/s]\u001b[A\n",
            " 27%|██▋       | 48/178 [00:02<00:07, 18.49it/s]\u001b[A\n",
            " 28%|██▊       | 50/178 [00:02<00:06, 18.48it/s]\u001b[A\n",
            " 29%|██▉       | 52/178 [00:02<00:06, 18.46it/s]\u001b[A\n",
            " 30%|███       | 54/178 [00:02<00:06, 18.45it/s]\u001b[A\n",
            " 31%|███▏      | 56/178 [00:03<00:06, 18.39it/s]\u001b[A\n",
            " 33%|███▎      | 58/178 [00:03<00:06, 18.48it/s]\u001b[A\n",
            " 34%|███▎      | 60/178 [00:03<00:06, 18.54it/s]\u001b[A\n",
            " 35%|███▍      | 62/178 [00:03<00:06, 18.63it/s]\u001b[A\n",
            " 36%|███▌      | 64/178 [00:03<00:06, 18.59it/s]\u001b[A\n",
            " 37%|███▋      | 66/178 [00:03<00:06, 18.66it/s]\u001b[A\n",
            " 38%|███▊      | 68/178 [00:03<00:05, 18.64it/s]\u001b[A\n",
            " 39%|███▉      | 70/178 [00:03<00:05, 18.56it/s]\u001b[A\n",
            " 40%|████      | 72/178 [00:03<00:05, 18.51it/s]\u001b[A\n",
            " 42%|████▏     | 74/178 [00:04<00:05, 18.56it/s]\u001b[A\n",
            " 43%|████▎     | 76/178 [00:04<00:05, 18.42it/s]\u001b[A\n",
            " 44%|████▍     | 78/178 [00:04<00:05, 18.49it/s]\u001b[A\n",
            " 45%|████▍     | 80/178 [00:04<00:05, 18.57it/s]\u001b[A\n",
            " 46%|████▌     | 82/178 [00:04<00:05, 18.59it/s]\u001b[A\n",
            " 47%|████▋     | 84/178 [00:04<00:05, 18.49it/s]\u001b[A\n",
            " 48%|████▊     | 86/178 [00:04<00:04, 18.49it/s]\u001b[A\n",
            " 49%|████▉     | 88/178 [00:04<00:04, 18.55it/s]\u001b[A\n",
            " 51%|█████     | 90/178 [00:04<00:04, 18.61it/s]\u001b[A\n",
            " 52%|█████▏    | 92/178 [00:04<00:04, 18.55it/s]\u001b[A\n",
            " 53%|█████▎    | 94/178 [00:05<00:04, 18.56it/s]\u001b[A\n",
            " 54%|█████▍    | 96/178 [00:05<00:04, 18.54it/s]\u001b[A\n",
            " 55%|█████▌    | 98/178 [00:05<00:04, 18.51it/s]\u001b[A\n",
            " 56%|█████▌    | 100/178 [00:05<00:04, 18.41it/s]\u001b[A\n",
            " 57%|█████▋    | 102/178 [00:05<00:04, 18.43it/s]\u001b[A\n",
            " 58%|█████▊    | 104/178 [00:05<00:04, 18.50it/s]\u001b[A\n",
            " 60%|█████▉    | 106/178 [00:05<00:03, 18.53it/s]\u001b[A\n",
            " 61%|██████    | 108/178 [00:05<00:03, 18.56it/s]\u001b[A\n",
            " 62%|██████▏   | 110/178 [00:05<00:03, 18.52it/s]\u001b[A\n",
            " 63%|██████▎   | 112/178 [00:06<00:03, 18.59it/s]\u001b[A\n",
            " 64%|██████▍   | 114/178 [00:06<00:03, 18.54it/s]\u001b[A\n",
            " 65%|██████▌   | 116/178 [00:06<00:03, 18.60it/s]\u001b[A\n",
            " 66%|██████▋   | 118/178 [00:06<00:03, 18.59it/s]\u001b[A\n",
            " 67%|██████▋   | 120/178 [00:06<00:03, 18.62it/s]\u001b[A\n",
            " 69%|██████▊   | 122/178 [00:06<00:02, 18.69it/s]\u001b[A\n",
            " 70%|██████▉   | 124/178 [00:06<00:02, 18.63it/s]\u001b[A\n",
            " 71%|███████   | 126/178 [00:06<00:02, 18.62it/s]\u001b[A\n",
            " 72%|███████▏  | 128/178 [00:06<00:02, 18.62it/s]\u001b[A\n",
            " 73%|███████▎  | 130/178 [00:07<00:02, 18.47it/s]\u001b[A\n",
            " 74%|███████▍  | 132/178 [00:07<00:02, 18.55it/s]\u001b[A\n",
            " 75%|███████▌  | 134/178 [00:07<00:02, 18.52it/s]\u001b[A\n",
            " 76%|███████▋  | 136/178 [00:07<00:02, 18.53it/s]\u001b[A\n",
            " 78%|███████▊  | 138/178 [00:07<00:02, 18.43it/s]\u001b[A\n",
            " 79%|███████▊  | 140/178 [00:07<00:02, 18.38it/s]\u001b[A\n",
            " 80%|███████▉  | 142/178 [00:07<00:01, 18.51it/s]\u001b[A\n",
            " 81%|████████  | 144/178 [00:07<00:01, 18.46it/s]\u001b[A\n",
            " 82%|████████▏ | 146/178 [00:07<00:01, 18.51it/s]\u001b[A\n",
            " 83%|████████▎ | 148/178 [00:08<00:01, 18.48it/s]\u001b[A\n",
            " 84%|████████▍ | 150/178 [00:08<00:01, 18.46it/s]\u001b[A\n",
            " 85%|████████▌ | 152/178 [00:08<00:01, 18.57it/s]\u001b[A\n",
            " 87%|████████▋ | 154/178 [00:08<00:01, 18.48it/s]\u001b[A\n",
            " 88%|████████▊ | 156/178 [00:08<00:01, 18.60it/s]\u001b[A\n",
            " 89%|████████▉ | 158/178 [00:08<00:01, 18.55it/s]\u001b[A\n",
            " 90%|████████▉ | 160/178 [00:08<00:00, 18.54it/s]\u001b[A\n",
            " 91%|█████████ | 162/178 [00:08<00:00, 18.54it/s]\u001b[A\n",
            " 92%|█████████▏| 164/178 [00:08<00:00, 18.52it/s]\u001b[A\n",
            " 93%|█████████▎| 166/178 [00:08<00:00, 18.55it/s]\u001b[A\n",
            " 94%|█████████▍| 168/178 [00:09<00:00, 18.56it/s]\u001b[A\n",
            " 96%|█████████▌| 170/178 [00:09<00:00, 18.48it/s]\u001b[A\n",
            " 97%|█████████▋| 172/178 [00:09<00:00, 18.42it/s]\u001b[A\n",
            " 98%|█████████▊| 174/178 [00:09<00:00, 18.36it/s]\u001b[A\n",
            "100%|██████████| 178/178 [00:09<00:00, 18.53it/s]\n",
            " 29%|██▊       | 406/1422 [01:45<52:15,  3.09s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            " 29%|██▊       | 407/1422 [01:46<37:31,  2.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.3209 Val acc: 0.9043 Val f1: 0.9043 AUC: 0.4823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 608/1422 [02:26<02:43,  4.99it/s]\n",
            "  0%|          | 0/178 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 2/178 [00:00<00:09, 18.33it/s]\u001b[A\n",
            "  2%|▏         | 4/178 [00:00<00:09, 18.22it/s]\u001b[A\n",
            "  3%|▎         | 6/178 [00:00<00:09, 18.22it/s]\u001b[A\n",
            "  4%|▍         | 8/178 [00:00<00:09, 18.36it/s]\u001b[A\n",
            "  6%|▌         | 10/178 [00:00<00:09, 18.37it/s]\u001b[A\n",
            "  7%|▋         | 12/178 [00:00<00:08, 18.49it/s]\u001b[A\n",
            "  8%|▊         | 14/178 [00:00<00:08, 18.34it/s]\u001b[A\n",
            "  9%|▉         | 16/178 [00:00<00:08, 18.28it/s]\u001b[A\n",
            " 10%|█         | 18/178 [00:00<00:08, 18.31it/s]\u001b[A\n",
            " 11%|█         | 20/178 [00:01<00:08, 18.37it/s]\u001b[A\n",
            " 12%|█▏        | 22/178 [00:01<00:08, 18.47it/s]\u001b[A\n",
            " 13%|█▎        | 24/178 [00:01<00:08, 18.51it/s]\u001b[A\n",
            " 15%|█▍        | 26/178 [00:01<00:08, 18.46it/s]\u001b[A\n",
            " 16%|█▌        | 28/178 [00:01<00:08, 18.38it/s]\u001b[A\n",
            " 17%|█▋        | 30/178 [00:01<00:08, 18.32it/s]\u001b[A\n",
            " 18%|█▊        | 32/178 [00:01<00:07, 18.34it/s]\u001b[A\n",
            " 19%|█▉        | 34/178 [00:01<00:07, 18.31it/s]\u001b[A\n",
            " 20%|██        | 36/178 [00:01<00:07, 18.28it/s]\u001b[A\n",
            " 21%|██▏       | 38/178 [00:02<00:07, 18.33it/s]\u001b[A\n",
            " 22%|██▏       | 40/178 [00:02<00:07, 18.42it/s]\u001b[A\n",
            " 24%|██▎       | 42/178 [00:02<00:07, 18.45it/s]\u001b[A\n",
            " 25%|██▍       | 44/178 [00:02<00:07, 18.46it/s]\u001b[A\n",
            " 26%|██▌       | 46/178 [00:02<00:07, 18.43it/s]\u001b[A\n",
            " 27%|██▋       | 48/178 [00:02<00:07, 18.50it/s]\u001b[A\n",
            " 28%|██▊       | 50/178 [00:02<00:06, 18.60it/s]\u001b[A\n",
            " 29%|██▉       | 52/178 [00:02<00:06, 18.47it/s]\u001b[A\n",
            " 30%|███       | 54/178 [00:02<00:06, 18.44it/s]\u001b[A\n",
            " 31%|███▏      | 56/178 [00:03<00:06, 18.51it/s]\u001b[A\n",
            " 33%|███▎      | 58/178 [00:03<00:06, 18.46it/s]\u001b[A\n",
            " 34%|███▎      | 60/178 [00:03<00:06, 18.49it/s]\u001b[A\n",
            " 35%|███▍      | 62/178 [00:03<00:06, 18.53it/s]\u001b[A\n",
            " 36%|███▌      | 64/178 [00:03<00:06, 18.42it/s]\u001b[A\n",
            " 37%|███▋      | 66/178 [00:03<00:06, 18.44it/s]\u001b[A\n",
            " 38%|███▊      | 68/178 [00:03<00:05, 18.46it/s]\u001b[A\n",
            " 39%|███▉      | 70/178 [00:03<00:05, 18.45it/s]\u001b[A\n",
            " 40%|████      | 72/178 [00:03<00:05, 18.50it/s]\u001b[A\n",
            " 42%|████▏     | 74/178 [00:04<00:05, 18.35it/s]\u001b[A\n",
            " 43%|████▎     | 76/178 [00:04<00:05, 18.33it/s]\u001b[A\n",
            " 44%|████▍     | 78/178 [00:04<00:05, 18.35it/s]\u001b[A\n",
            " 45%|████▍     | 80/178 [00:04<00:05, 18.43it/s]\u001b[A\n",
            " 46%|████▌     | 82/178 [00:04<00:05, 18.49it/s]\u001b[A\n",
            " 47%|████▋     | 84/178 [00:04<00:05, 18.56it/s]\u001b[A\n",
            " 48%|████▊     | 86/178 [00:04<00:04, 18.56it/s]\u001b[A\n",
            " 49%|████▉     | 88/178 [00:04<00:04, 18.55it/s]\u001b[A\n",
            " 51%|█████     | 90/178 [00:04<00:04, 18.50it/s]\u001b[A\n",
            " 52%|█████▏    | 92/178 [00:04<00:04, 18.47it/s]\u001b[A\n",
            " 53%|█████▎    | 94/178 [00:05<00:04, 18.42it/s]\u001b[A\n",
            " 54%|█████▍    | 96/178 [00:05<00:04, 18.46it/s]\u001b[A\n",
            " 55%|█████▌    | 98/178 [00:05<00:04, 18.47it/s]\u001b[A\n",
            " 56%|█████▌    | 100/178 [00:05<00:04, 18.38it/s]\u001b[A\n",
            " 57%|█████▋    | 102/178 [00:05<00:04, 18.44it/s]\u001b[A\n",
            " 58%|█████▊    | 104/178 [00:05<00:04, 18.36it/s]\u001b[A\n",
            " 60%|█████▉    | 106/178 [00:05<00:03, 18.48it/s]\u001b[A\n",
            " 61%|██████    | 108/178 [00:05<00:03, 18.38it/s]\u001b[A\n",
            " 62%|██████▏   | 110/178 [00:05<00:03, 18.35it/s]\u001b[A\n",
            " 63%|██████▎   | 112/178 [00:06<00:03, 18.34it/s]\u001b[A\n",
            " 64%|██████▍   | 114/178 [00:06<00:03, 18.44it/s]\u001b[A\n",
            " 65%|██████▌   | 116/178 [00:06<00:03, 18.50it/s]\u001b[A\n",
            " 66%|██████▋   | 118/178 [00:06<00:03, 18.44it/s]\u001b[A\n",
            " 67%|██████▋   | 120/178 [00:06<00:03, 18.41it/s]\u001b[A\n",
            " 69%|██████▊   | 122/178 [00:06<00:03, 18.45it/s]\u001b[A\n",
            " 70%|██████▉   | 124/178 [00:06<00:02, 18.49it/s]\u001b[A\n",
            " 71%|███████   | 126/178 [00:06<00:02, 18.53it/s]\u001b[A\n",
            " 72%|███████▏  | 128/178 [00:06<00:02, 18.60it/s]\u001b[A\n",
            " 73%|███████▎  | 130/178 [00:07<00:02, 18.52it/s]\u001b[A\n",
            " 74%|███████▍  | 132/178 [00:07<00:02, 18.57it/s]\u001b[A\n",
            " 75%|███████▌  | 134/178 [00:07<00:02, 18.58it/s]\u001b[A\n",
            " 76%|███████▋  | 136/178 [00:07<00:02, 18.60it/s]\u001b[A\n",
            " 78%|███████▊  | 138/178 [00:07<00:02, 18.48it/s]\u001b[A\n",
            " 79%|███████▊  | 140/178 [00:07<00:02, 18.52it/s]\u001b[A\n",
            " 80%|███████▉  | 142/178 [00:07<00:01, 18.56it/s]\u001b[A\n",
            " 81%|████████  | 144/178 [00:07<00:01, 18.58it/s]\u001b[A\n",
            " 82%|████████▏ | 146/178 [00:07<00:01, 18.56it/s]\u001b[A\n",
            " 83%|████████▎ | 148/178 [00:08<00:01, 18.41it/s]\u001b[A\n",
            " 84%|████████▍ | 150/178 [00:08<00:01, 18.43it/s]\u001b[A\n",
            " 85%|████████▌ | 152/178 [00:08<00:01, 18.43it/s]\u001b[A\n",
            " 87%|████████▋ | 154/178 [00:08<00:01, 18.46it/s]\u001b[A\n",
            " 88%|████████▊ | 156/178 [00:08<00:01, 18.55it/s]\u001b[A\n",
            " 89%|████████▉ | 158/178 [00:08<00:01, 18.63it/s]\u001b[A\n",
            " 90%|████████▉ | 160/178 [00:08<00:00, 18.54it/s]\u001b[A\n",
            " 91%|█████████ | 162/178 [00:08<00:00, 18.50it/s]\u001b[A\n",
            " 92%|█████████▏| 164/178 [00:08<00:00, 18.45it/s]\u001b[A\n",
            " 93%|█████████▎| 166/178 [00:08<00:00, 18.43it/s]\u001b[A\n",
            " 94%|█████████▍| 168/178 [00:09<00:00, 18.41it/s]\u001b[A\n",
            " 96%|█████████▌| 170/178 [00:09<00:00, 18.55it/s]\u001b[A\n",
            " 97%|█████████▋| 172/178 [00:09<00:00, 18.55it/s]\u001b[A\n",
            " 98%|█████████▊| 174/178 [00:09<00:00, 18.56it/s]\u001b[A\n",
            "100%|██████████| 178/178 [00:09<00:00, 18.48it/s]\n",
            " 43%|████▎     | 609/1422 [02:36<41:54,  3.09s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            " 43%|████▎     | 610/1422 [02:36<30:04,  2.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.3181 Val acc: 0.9043 Val f1: 0.9043 AUC: 0.4915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 811/1422 [03:16<02:02,  4.99it/s]\n",
            "  0%|          | 0/178 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 2/178 [00:00<00:09, 18.17it/s]\u001b[A\n",
            "  2%|▏         | 4/178 [00:00<00:09, 18.34it/s]\u001b[A\n",
            "  3%|▎         | 6/178 [00:00<00:09, 18.50it/s]\u001b[A\n",
            "  4%|▍         | 8/178 [00:00<00:09, 18.47it/s]\u001b[A\n",
            "  6%|▌         | 10/178 [00:00<00:09, 18.60it/s]\u001b[A\n",
            "  7%|▋         | 12/178 [00:00<00:08, 18.58it/s]\u001b[A\n",
            "  8%|▊         | 14/178 [00:00<00:08, 18.58it/s]\u001b[A\n",
            "  9%|▉         | 16/178 [00:00<00:08, 18.58it/s]\u001b[A\n",
            " 10%|█         | 18/178 [00:00<00:08, 18.53it/s]\u001b[A\n",
            " 11%|█         | 20/178 [00:01<00:08, 18.49it/s]\u001b[A\n",
            " 12%|█▏        | 22/178 [00:01<00:08, 18.59it/s]\u001b[A\n",
            " 13%|█▎        | 24/178 [00:01<00:08, 18.55it/s]\u001b[A\n",
            " 15%|█▍        | 26/178 [00:01<00:08, 18.46it/s]\u001b[A\n",
            " 16%|█▌        | 28/178 [00:01<00:08, 18.41it/s]\u001b[A\n",
            " 17%|█▋        | 30/178 [00:01<00:08, 18.41it/s]\u001b[A\n",
            " 18%|█▊        | 32/178 [00:01<00:07, 18.41it/s]\u001b[A\n",
            " 19%|█▉        | 34/178 [00:01<00:07, 18.50it/s]\u001b[A\n",
            " 20%|██        | 36/178 [00:01<00:07, 18.41it/s]\u001b[A\n",
            " 21%|██▏       | 38/178 [00:02<00:07, 18.50it/s]\u001b[A\n",
            " 22%|██▏       | 40/178 [00:02<00:07, 18.53it/s]\u001b[A\n",
            " 24%|██▎       | 42/178 [00:02<00:07, 18.51it/s]\u001b[A\n",
            " 25%|██▍       | 44/178 [00:02<00:07, 18.52it/s]\u001b[A\n",
            " 26%|██▌       | 46/178 [00:02<00:07, 18.50it/s]\u001b[A\n",
            " 27%|██▋       | 48/178 [00:02<00:06, 18.58it/s]\u001b[A\n",
            " 28%|██▊       | 50/178 [00:02<00:06, 18.55it/s]\u001b[A\n",
            " 29%|██▉       | 52/178 [00:02<00:06, 18.48it/s]\u001b[A\n",
            " 30%|███       | 54/178 [00:02<00:06, 18.49it/s]\u001b[A\n",
            " 31%|███▏      | 56/178 [00:03<00:06, 18.53it/s]\u001b[A\n",
            " 33%|███▎      | 58/178 [00:03<00:06, 18.46it/s]\u001b[A\n",
            " 34%|███▎      | 60/178 [00:03<00:06, 18.45it/s]\u001b[A\n",
            " 35%|███▍      | 62/178 [00:03<00:06, 18.48it/s]\u001b[A\n",
            " 36%|███▌      | 64/178 [00:03<00:06, 18.54it/s]\u001b[A\n",
            " 37%|███▋      | 66/178 [00:03<00:06, 18.49it/s]\u001b[A\n",
            " 38%|███▊      | 68/178 [00:03<00:05, 18.51it/s]\u001b[A\n",
            " 39%|███▉      | 70/178 [00:03<00:05, 18.47it/s]\u001b[A\n",
            " 40%|████      | 72/178 [00:03<00:05, 18.40it/s]\u001b[A\n",
            " 42%|████▏     | 74/178 [00:04<00:05, 18.40it/s]\u001b[A\n",
            " 43%|████▎     | 76/178 [00:04<00:05, 18.38it/s]\u001b[A\n",
            " 44%|████▍     | 78/178 [00:04<00:05, 18.40it/s]\u001b[A\n",
            " 45%|████▍     | 80/178 [00:04<00:05, 18.45it/s]\u001b[A\n",
            " 46%|████▌     | 82/178 [00:04<00:05, 18.45it/s]\u001b[A\n",
            " 47%|████▋     | 84/178 [00:04<00:05, 18.41it/s]\u001b[A\n",
            " 48%|████▊     | 86/178 [00:04<00:04, 18.46it/s]\u001b[A\n",
            " 49%|████▉     | 88/178 [00:04<00:04, 18.47it/s]\u001b[A\n",
            " 51%|█████     | 90/178 [00:04<00:04, 18.49it/s]\u001b[A\n",
            " 52%|█████▏    | 92/178 [00:04<00:04, 18.59it/s]\u001b[A\n",
            " 53%|█████▎    | 94/178 [00:05<00:04, 18.58it/s]\u001b[A\n",
            " 54%|█████▍    | 96/178 [00:05<00:04, 18.56it/s]\u001b[A\n",
            " 55%|█████▌    | 98/178 [00:05<00:04, 18.57it/s]\u001b[A\n",
            " 56%|█████▌    | 100/178 [00:05<00:04, 18.57it/s]\u001b[A\n",
            " 57%|█████▋    | 102/178 [00:05<00:04, 18.56it/s]\u001b[A\n",
            " 58%|█████▊    | 104/178 [00:05<00:03, 18.53it/s]\u001b[A\n",
            " 60%|█████▉    | 106/178 [00:05<00:03, 18.56it/s]\u001b[A\n",
            " 61%|██████    | 108/178 [00:05<00:03, 18.54it/s]\u001b[A\n",
            " 62%|██████▏   | 110/178 [00:05<00:03, 18.62it/s]\u001b[A\n",
            " 63%|██████▎   | 112/178 [00:06<00:03, 18.53it/s]\u001b[A\n",
            " 64%|██████▍   | 114/178 [00:06<00:03, 18.52it/s]\u001b[A\n",
            " 65%|██████▌   | 116/178 [00:06<00:03, 18.54it/s]\u001b[A\n",
            " 66%|██████▋   | 118/178 [00:06<00:03, 18.58it/s]\u001b[A\n",
            " 67%|██████▋   | 120/178 [00:06<00:03, 18.35it/s]\u001b[A\n",
            " 69%|██████▊   | 122/178 [00:06<00:03, 18.41it/s]\u001b[A\n",
            " 70%|██████▉   | 124/178 [00:06<00:02, 18.35it/s]\u001b[A\n",
            " 71%|███████   | 126/178 [00:06<00:02, 18.44it/s]\u001b[A\n",
            " 72%|███████▏  | 128/178 [00:06<00:02, 18.46it/s]\u001b[A\n",
            " 73%|███████▎  | 130/178 [00:07<00:02, 18.50it/s]\u001b[A\n",
            " 74%|███████▍  | 132/178 [00:07<00:02, 18.52it/s]\u001b[A\n",
            " 75%|███████▌  | 134/178 [00:07<00:02, 18.42it/s]\u001b[A\n",
            " 76%|███████▋  | 136/178 [00:07<00:02, 18.45it/s]\u001b[A\n",
            " 78%|███████▊  | 138/178 [00:07<00:02, 18.45it/s]\u001b[A\n",
            " 79%|███████▊  | 140/178 [00:07<00:02, 18.49it/s]\u001b[A\n",
            " 80%|███████▉  | 142/178 [00:07<00:01, 18.55it/s]\u001b[A\n",
            " 81%|████████  | 144/178 [00:07<00:01, 18.58it/s]\u001b[A\n",
            " 82%|████████▏ | 146/178 [00:07<00:01, 18.53it/s]\u001b[A\n",
            " 83%|████████▎ | 148/178 [00:08<00:01, 18.49it/s]\u001b[A\n",
            " 84%|████████▍ | 150/178 [00:08<00:01, 18.50it/s]\u001b[A\n",
            " 85%|████████▌ | 152/178 [00:08<00:01, 18.50it/s]\u001b[A\n",
            " 87%|████████▋ | 154/178 [00:08<00:01, 18.52it/s]\u001b[A\n",
            " 88%|████████▊ | 156/178 [00:08<00:01, 18.46it/s]\u001b[A\n",
            " 89%|████████▉ | 158/178 [00:08<00:01, 18.49it/s]\u001b[A\n",
            " 90%|████████▉ | 160/178 [00:08<00:00, 18.50it/s]\u001b[A\n",
            " 91%|█████████ | 162/178 [00:08<00:00, 18.51it/s]\u001b[A\n",
            " 92%|█████████▏| 164/178 [00:08<00:00, 18.41it/s]\u001b[A\n",
            " 93%|█████████▎| 166/178 [00:08<00:00, 18.46it/s]\u001b[A\n",
            " 94%|█████████▍| 168/178 [00:09<00:00, 18.47it/s]\u001b[A\n",
            " 96%|█████████▌| 170/178 [00:09<00:00, 18.50it/s]\u001b[A\n",
            " 97%|█████████▋| 172/178 [00:09<00:00, 18.49it/s]\u001b[A\n",
            " 98%|█████████▊| 174/178 [00:09<00:00, 18.54it/s]\u001b[A\n",
            "100%|██████████| 178/178 [00:09<00:00, 18.52it/s]\n",
            " 57%|█████▋    | 812/1422 [03:26<31:22,  3.09s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            " 57%|█████▋    | 813/1422 [03:26<22:31,  2.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.3221 Val acc: 0.9043 Val f1: 0.9043 AUC: 0.4808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████▏  | 1014/1422 [04:07<01:21,  4.99it/s]\n",
            "  0%|          | 0/178 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 2/178 [00:00<00:09, 18.13it/s]\u001b[A\n",
            "  2%|▏         | 4/178 [00:00<00:09, 18.18it/s]\u001b[A\n",
            "  3%|▎         | 6/178 [00:00<00:09, 18.31it/s]\u001b[A\n",
            "  4%|▍         | 8/178 [00:00<00:09, 18.40it/s]\u001b[A\n",
            "  6%|▌         | 10/178 [00:00<00:09, 18.49it/s]\u001b[A\n",
            "  7%|▋         | 12/178 [00:00<00:09, 18.35it/s]\u001b[A\n",
            "  8%|▊         | 14/178 [00:00<00:08, 18.39it/s]\u001b[A\n",
            "  9%|▉         | 16/178 [00:00<00:08, 18.46it/s]\u001b[A\n",
            " 10%|█         | 18/178 [00:00<00:08, 18.48it/s]\u001b[A\n",
            " 11%|█         | 20/178 [00:01<00:08, 18.54it/s]\u001b[A\n",
            " 12%|█▏        | 22/178 [00:01<00:08, 18.46it/s]\u001b[A\n",
            " 13%|█▎        | 24/178 [00:01<00:08, 18.48it/s]\u001b[A\n",
            " 15%|█▍        | 26/178 [00:01<00:08, 18.53it/s]\u001b[A\n",
            " 16%|█▌        | 28/178 [00:01<00:08, 18.50it/s]\u001b[A\n",
            " 17%|█▋        | 30/178 [00:01<00:07, 18.53it/s]\u001b[A\n",
            " 18%|█▊        | 32/178 [00:01<00:07, 18.55it/s]\u001b[A\n",
            " 19%|█▉        | 34/178 [00:01<00:07, 18.53it/s]\u001b[A\n",
            " 20%|██        | 36/178 [00:01<00:07, 18.46it/s]\u001b[A\n",
            " 21%|██▏       | 38/178 [00:02<00:07, 18.41it/s]\u001b[A\n",
            " 22%|██▏       | 40/178 [00:02<00:07, 18.45it/s]\u001b[A\n",
            " 24%|██▎       | 42/178 [00:02<00:07, 18.47it/s]\u001b[A\n",
            " 25%|██▍       | 44/178 [00:02<00:07, 18.41it/s]\u001b[A\n",
            " 26%|██▌       | 46/178 [00:02<00:07, 18.45it/s]\u001b[A\n",
            " 27%|██▋       | 48/178 [00:02<00:07, 18.47it/s]\u001b[A\n",
            " 28%|██▊       | 50/178 [00:02<00:06, 18.49it/s]\u001b[A\n",
            " 29%|██▉       | 52/178 [00:02<00:06, 18.36it/s]\u001b[A\n",
            " 30%|███       | 54/178 [00:02<00:06, 18.28it/s]\u001b[A\n",
            " 31%|███▏      | 56/178 [00:03<00:06, 18.31it/s]\u001b[A\n",
            " 33%|███▎      | 58/178 [00:03<00:06, 18.29it/s]\u001b[A\n",
            " 34%|███▎      | 60/178 [00:03<00:06, 18.35it/s]\u001b[A\n",
            " 35%|███▍      | 62/178 [00:03<00:06, 18.20it/s]\u001b[A\n",
            " 36%|███▌      | 64/178 [00:03<00:06, 18.32it/s]\u001b[A\n",
            " 37%|███▋      | 66/178 [00:03<00:06, 18.40it/s]\u001b[A\n",
            " 38%|███▊      | 68/178 [00:03<00:05, 18.45it/s]\u001b[A\n",
            " 39%|███▉      | 70/178 [00:03<00:05, 18.43it/s]\u001b[A\n",
            " 40%|████      | 72/178 [00:03<00:05, 18.27it/s]\u001b[A\n",
            " 42%|████▏     | 74/178 [00:04<00:05, 18.37it/s]\u001b[A\n",
            " 43%|████▎     | 76/178 [00:04<00:05, 18.46it/s]\u001b[A\n",
            " 44%|████▍     | 78/178 [00:04<00:05, 18.46it/s]\u001b[A\n",
            " 45%|████▍     | 80/178 [00:04<00:05, 18.31it/s]\u001b[A\n",
            " 46%|████▌     | 82/178 [00:04<00:05, 18.34it/s]\u001b[A\n",
            " 47%|████▋     | 84/178 [00:04<00:05, 18.34it/s]\u001b[A\n",
            " 48%|████▊     | 86/178 [00:04<00:05, 18.39it/s]\u001b[A\n",
            " 49%|████▉     | 88/178 [00:04<00:04, 18.36it/s]\u001b[A\n",
            " 51%|█████     | 90/178 [00:04<00:04, 18.39it/s]\u001b[A\n",
            " 52%|█████▏    | 92/178 [00:04<00:04, 18.49it/s]\u001b[A\n",
            " 53%|█████▎    | 94/178 [00:05<00:04, 18.51it/s]\u001b[A\n",
            " 54%|█████▍    | 96/178 [00:05<00:04, 18.48it/s]\u001b[A\n",
            " 55%|█████▌    | 98/178 [00:05<00:04, 18.48it/s]\u001b[A\n",
            " 56%|█████▌    | 100/178 [00:05<00:04, 18.45it/s]\u001b[A\n",
            " 57%|█████▋    | 102/178 [00:05<00:04, 18.50it/s]\u001b[A\n",
            " 58%|█████▊    | 104/178 [00:05<00:03, 18.56it/s]\u001b[A\n",
            " 60%|█████▉    | 106/178 [00:05<00:03, 18.60it/s]\u001b[A\n",
            " 61%|██████    | 108/178 [00:05<00:03, 18.38it/s]\u001b[A\n",
            " 62%|██████▏   | 110/178 [00:05<00:03, 18.31it/s]\u001b[A\n",
            " 63%|██████▎   | 112/178 [00:06<00:03, 18.44it/s]\u001b[A\n",
            " 64%|██████▍   | 114/178 [00:06<00:03, 18.42it/s]\u001b[A\n",
            " 65%|██████▌   | 116/178 [00:06<00:03, 18.40it/s]\u001b[A\n",
            " 66%|██████▋   | 118/178 [00:06<00:03, 18.34it/s]\u001b[A\n",
            " 67%|██████▋   | 120/178 [00:06<00:03, 18.42it/s]\u001b[A\n",
            " 69%|██████▊   | 122/178 [00:06<00:03, 18.39it/s]\u001b[A\n",
            " 70%|██████▉   | 124/178 [00:06<00:02, 18.33it/s]\u001b[A\n",
            " 71%|███████   | 126/178 [00:06<00:02, 18.34it/s]\u001b[A\n",
            " 72%|███████▏  | 128/178 [00:06<00:02, 18.37it/s]\u001b[A\n",
            " 73%|███████▎  | 130/178 [00:07<00:02, 18.22it/s]\u001b[A\n",
            " 74%|███████▍  | 132/178 [00:07<00:02, 18.29it/s]\u001b[A\n",
            " 75%|███████▌  | 134/178 [00:07<00:02, 18.32it/s]\u001b[A\n",
            " 76%|███████▋  | 136/178 [00:07<00:02, 18.25it/s]\u001b[A\n",
            " 78%|███████▊  | 138/178 [00:07<00:02, 18.35it/s]\u001b[A\n",
            " 79%|███████▊  | 140/178 [00:07<00:02, 18.35it/s]\u001b[A\n",
            " 80%|███████▉  | 142/178 [00:07<00:01, 18.35it/s]\u001b[A\n",
            " 81%|████████  | 144/178 [00:07<00:01, 18.45it/s]\u001b[A\n",
            " 82%|████████▏ | 146/178 [00:07<00:01, 18.50it/s]\u001b[A\n",
            " 83%|████████▎ | 148/178 [00:08<00:01, 18.49it/s]\u001b[A\n",
            " 84%|████████▍ | 150/178 [00:08<00:01, 18.50it/s]\u001b[A\n",
            " 85%|████████▌ | 152/178 [00:08<00:01, 18.35it/s]\u001b[A\n",
            " 87%|████████▋ | 154/178 [00:08<00:01, 18.42it/s]\u001b[A\n",
            " 88%|████████▊ | 156/178 [00:08<00:01, 18.50it/s]\u001b[A\n",
            " 89%|████████▉ | 158/178 [00:08<00:01, 18.50it/s]\u001b[A\n",
            " 90%|████████▉ | 160/178 [00:08<00:00, 18.37it/s]\u001b[A\n",
            " 91%|█████████ | 162/178 [00:08<00:00, 18.39it/s]\u001b[A\n",
            " 92%|█████████▏| 164/178 [00:08<00:00, 18.26it/s]\u001b[A\n",
            " 93%|█████████▎| 166/178 [00:09<00:00, 18.27it/s]\u001b[A\n",
            " 94%|█████████▍| 168/178 [00:09<00:00, 18.35it/s]\u001b[A\n",
            " 96%|█████████▌| 170/178 [00:09<00:00, 18.44it/s]\u001b[A\n",
            " 97%|█████████▋| 172/178 [00:09<00:00, 18.42it/s]\u001b[A\n",
            " 98%|█████████▊| 174/178 [00:09<00:00, 18.33it/s]\u001b[A\n",
            "100%|██████████| 178/178 [00:09<00:00, 18.43it/s]\n",
            " 71%|███████▏  | 1015/1422 [04:16<21:02,  3.10s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            " 71%|███████▏  | 1016/1422 [04:17<15:05,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.3173 Val acc: 0.9043 Val f1: 0.9043 AUC: 0.5391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 1217/1422 [04:57<00:41,  5.00it/s]\n",
            "  0%|          | 0/178 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 2/178 [00:00<00:09, 18.30it/s]\u001b[A\n",
            "  2%|▏         | 4/178 [00:00<00:09, 18.36it/s]\u001b[A\n",
            "  3%|▎         | 6/178 [00:00<00:09, 18.28it/s]\u001b[A\n",
            "  4%|▍         | 8/178 [00:00<00:09, 18.45it/s]\u001b[A\n",
            "  6%|▌         | 10/178 [00:00<00:09, 18.38it/s]\u001b[A\n",
            "  7%|▋         | 12/178 [00:00<00:08, 18.49it/s]\u001b[A\n",
            "  8%|▊         | 14/178 [00:00<00:08, 18.46it/s]\u001b[A\n",
            "  9%|▉         | 16/178 [00:00<00:08, 18.57it/s]\u001b[A\n",
            " 10%|█         | 18/178 [00:00<00:08, 18.52it/s]\u001b[A\n",
            " 11%|█         | 20/178 [00:01<00:08, 18.52it/s]\u001b[A\n",
            " 12%|█▏        | 22/178 [00:01<00:08, 18.51it/s]\u001b[A\n",
            " 13%|█▎        | 24/178 [00:01<00:08, 18.22it/s]\u001b[A\n",
            " 15%|█▍        | 26/178 [00:01<00:08, 18.29it/s]\u001b[A\n",
            " 16%|█▌        | 28/178 [00:01<00:08, 18.27it/s]\u001b[A\n",
            " 17%|█▋        | 30/178 [00:01<00:08, 18.34it/s]\u001b[A\n",
            " 18%|█▊        | 32/178 [00:01<00:07, 18.37it/s]\u001b[A\n",
            " 19%|█▉        | 34/178 [00:01<00:07, 18.43it/s]\u001b[A\n",
            " 20%|██        | 36/178 [00:01<00:07, 18.42it/s]\u001b[A\n",
            " 21%|██▏       | 38/178 [00:02<00:07, 18.45it/s]\u001b[A\n",
            " 22%|██▏       | 40/178 [00:02<00:07, 18.45it/s]\u001b[A\n",
            " 24%|██▎       | 42/178 [00:02<00:07, 18.47it/s]\u001b[A\n",
            " 25%|██▍       | 44/178 [00:02<00:07, 18.42it/s]\u001b[A\n",
            " 26%|██▌       | 46/178 [00:02<00:07, 18.45it/s]\u001b[A\n",
            " 27%|██▋       | 48/178 [00:02<00:07, 18.54it/s]\u001b[A\n",
            " 28%|██▊       | 50/178 [00:02<00:06, 18.48it/s]\u001b[A\n",
            " 29%|██▉       | 52/178 [00:02<00:06, 18.42it/s]\u001b[A\n",
            " 30%|███       | 54/178 [00:02<00:06, 18.43it/s]\u001b[A\n",
            " 31%|███▏      | 56/178 [00:03<00:06, 18.40it/s]\u001b[A\n",
            " 33%|███▎      | 58/178 [00:03<00:06, 18.50it/s]\u001b[A\n",
            " 34%|███▎      | 60/178 [00:03<00:06, 18.55it/s]\u001b[A\n",
            " 35%|███▍      | 62/178 [00:03<00:06, 18.49it/s]\u001b[A\n",
            " 36%|███▌      | 64/178 [00:03<00:06, 18.43it/s]\u001b[A\n",
            " 37%|███▋      | 66/178 [00:03<00:06, 18.40it/s]\u001b[A\n",
            " 38%|███▊      | 68/178 [00:03<00:05, 18.39it/s]\u001b[A\n",
            " 39%|███▉      | 70/178 [00:03<00:05, 18.41it/s]\u001b[A\n",
            " 40%|████      | 72/178 [00:03<00:05, 18.42it/s]\u001b[A\n",
            " 42%|████▏     | 74/178 [00:04<00:05, 18.47it/s]\u001b[A\n",
            " 43%|████▎     | 76/178 [00:04<00:05, 18.51it/s]\u001b[A\n",
            " 44%|████▍     | 78/178 [00:04<00:05, 18.25it/s]\u001b[A\n",
            " 45%|████▍     | 80/178 [00:04<00:05, 18.41it/s]\u001b[A\n",
            " 46%|████▌     | 82/178 [00:04<00:05, 18.49it/s]\u001b[A\n",
            " 47%|████▋     | 84/178 [00:04<00:05, 18.54it/s]\u001b[A\n",
            " 48%|████▊     | 86/178 [00:04<00:04, 18.58it/s]\u001b[A\n",
            " 49%|████▉     | 88/178 [00:04<00:04, 18.49it/s]\u001b[A\n",
            " 51%|█████     | 90/178 [00:04<00:04, 18.46it/s]\u001b[A\n",
            " 52%|█████▏    | 92/178 [00:04<00:04, 18.48it/s]\u001b[A\n",
            " 53%|█████▎    | 94/178 [00:05<00:04, 18.53it/s]\u001b[A\n",
            " 54%|█████▍    | 96/178 [00:05<00:04, 18.42it/s]\u001b[A\n",
            " 55%|█████▌    | 98/178 [00:05<00:04, 18.39it/s]\u001b[A\n",
            " 56%|█████▌    | 100/178 [00:05<00:04, 18.37it/s]\u001b[A\n",
            " 57%|█████▋    | 102/178 [00:05<00:04, 18.42it/s]\u001b[A\n",
            " 58%|█████▊    | 104/178 [00:05<00:04, 18.47it/s]\u001b[A\n",
            " 60%|█████▉    | 106/178 [00:05<00:03, 18.36it/s]\u001b[A\n",
            " 61%|██████    | 108/178 [00:05<00:03, 18.38it/s]\u001b[A\n",
            " 62%|██████▏   | 110/178 [00:05<00:03, 18.42it/s]\u001b[A\n",
            " 63%|██████▎   | 112/178 [00:06<00:03, 18.41it/s]\u001b[A\n",
            " 64%|██████▍   | 114/178 [00:06<00:03, 18.45it/s]\u001b[A\n",
            " 65%|██████▌   | 116/178 [00:06<00:03, 18.25it/s]\u001b[A\n",
            " 66%|██████▋   | 118/178 [00:06<00:03, 18.34it/s]\u001b[A\n",
            " 67%|██████▋   | 120/178 [00:06<00:03, 18.29it/s]\u001b[A\n",
            " 69%|██████▊   | 122/178 [00:06<00:03, 18.42it/s]\u001b[A\n",
            " 70%|██████▉   | 124/178 [00:06<00:02, 18.41it/s]\u001b[A\n",
            " 71%|███████   | 126/178 [00:06<00:02, 18.47it/s]\u001b[A\n",
            " 72%|███████▏  | 128/178 [00:06<00:02, 18.50it/s]\u001b[A\n",
            " 73%|███████▎  | 130/178 [00:07<00:02, 18.52it/s]\u001b[A\n",
            " 74%|███████▍  | 132/178 [00:07<00:02, 18.44it/s]\u001b[A\n",
            " 75%|███████▌  | 134/178 [00:07<00:02, 18.34it/s]\u001b[A\n",
            " 76%|███████▋  | 136/178 [00:07<00:02, 18.34it/s]\u001b[A\n",
            " 78%|███████▊  | 138/178 [00:07<00:02, 18.32it/s]\u001b[A\n",
            " 79%|███████▊  | 140/178 [00:07<00:02, 18.29it/s]\u001b[A\n",
            " 80%|███████▉  | 142/178 [00:07<00:01, 18.37it/s]\u001b[A\n",
            " 81%|████████  | 144/178 [00:07<00:01, 18.41it/s]\u001b[A\n",
            " 82%|████████▏ | 146/178 [00:07<00:01, 18.54it/s]\u001b[A\n",
            " 83%|████████▎ | 148/178 [00:08<00:01, 18.54it/s]\u001b[A\n",
            " 84%|████████▍ | 150/178 [00:08<00:01, 18.49it/s]\u001b[A\n",
            " 85%|████████▌ | 152/178 [00:08<00:01, 18.49it/s]\u001b[A\n",
            " 87%|████████▋ | 154/178 [00:08<00:01, 18.48it/s]\u001b[A\n",
            " 88%|████████▊ | 156/178 [00:08<00:01, 18.46it/s]\u001b[A\n",
            " 89%|████████▉ | 158/178 [00:08<00:01, 18.51it/s]\u001b[A\n",
            " 90%|████████▉ | 160/178 [00:08<00:00, 18.42it/s]\u001b[A\n",
            " 91%|█████████ | 162/178 [00:08<00:00, 18.37it/s]\u001b[A\n",
            " 92%|█████████▏| 164/178 [00:08<00:00, 18.39it/s]\u001b[A\n",
            " 93%|█████████▎| 166/178 [00:09<00:00, 18.38it/s]\u001b[A\n",
            " 94%|█████████▍| 168/178 [00:09<00:00, 18.44it/s]\u001b[A\n",
            " 96%|█████████▌| 170/178 [00:09<00:00, 18.48it/s]\u001b[A\n",
            " 97%|█████████▋| 172/178 [00:09<00:00, 18.45it/s]\u001b[A\n",
            " 98%|█████████▊| 174/178 [00:09<00:00, 18.47it/s]\u001b[A\n",
            "100%|██████████| 178/178 [00:09<00:00, 18.46it/s]\n",
            " 86%|████████▌ | 1218/1422 [05:07<10:31,  3.10s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            " 86%|████████▌ | 1219/1422 [05:07<07:31,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.3207 Val acc: 0.9043 Val f1: 0.9043 AUC: 0.4208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1420/1422 [05:47<00:00,  5.00it/s]\n",
            "  0%|          | 0/178 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 2/178 [00:00<00:09, 18.49it/s]\u001b[A\n",
            "  2%|▏         | 4/178 [00:00<00:09, 18.52it/s]\u001b[A\n",
            "  3%|▎         | 6/178 [00:00<00:09, 18.56it/s]\u001b[A\n",
            "  4%|▍         | 8/178 [00:00<00:09, 18.51it/s]\u001b[A\n",
            "  6%|▌         | 10/178 [00:00<00:09, 18.49it/s]\u001b[A\n",
            "  7%|▋         | 12/178 [00:00<00:08, 18.52it/s]\u001b[A\n",
            "  8%|▊         | 14/178 [00:00<00:08, 18.48it/s]\u001b[A\n",
            "  9%|▉         | 16/178 [00:00<00:08, 18.50it/s]\u001b[A\n",
            " 10%|█         | 18/178 [00:00<00:08, 18.59it/s]\u001b[A\n",
            " 11%|█         | 20/178 [00:01<00:08, 18.52it/s]\u001b[A\n",
            " 12%|█▏        | 22/178 [00:01<00:08, 18.51it/s]\u001b[A\n",
            " 13%|█▎        | 24/178 [00:01<00:08, 18.50it/s]\u001b[A\n",
            " 15%|█▍        | 26/178 [00:01<00:08, 18.48it/s]\u001b[A\n",
            " 16%|█▌        | 28/178 [00:01<00:08, 18.47it/s]\u001b[A\n",
            " 17%|█▋        | 30/178 [00:01<00:07, 18.54it/s]\u001b[A\n",
            " 18%|█▊        | 32/178 [00:01<00:07, 18.46it/s]\u001b[A\n",
            " 19%|█▉        | 34/178 [00:01<00:07, 18.40it/s]\u001b[A\n",
            " 20%|██        | 36/178 [00:01<00:07, 18.40it/s]\u001b[A\n",
            " 21%|██▏       | 38/178 [00:02<00:07, 18.50it/s]\u001b[A\n",
            " 22%|██▏       | 40/178 [00:02<00:07, 18.57it/s]\u001b[A\n",
            " 24%|██▎       | 42/178 [00:02<00:07, 18.60it/s]\u001b[A\n",
            " 25%|██▍       | 44/178 [00:02<00:07, 18.40it/s]\u001b[A\n",
            " 26%|██▌       | 46/178 [00:02<00:07, 18.42it/s]\u001b[A\n",
            " 27%|██▋       | 48/178 [00:02<00:07, 18.43it/s]\u001b[A\n",
            " 28%|██▊       | 50/178 [00:02<00:06, 18.47it/s]\u001b[A\n",
            " 29%|██▉       | 52/178 [00:02<00:06, 18.55it/s]\u001b[A\n",
            " 30%|███       | 54/178 [00:02<00:06, 18.57it/s]\u001b[A\n",
            " 31%|███▏      | 56/178 [00:03<00:06, 18.55it/s]\u001b[A\n",
            " 33%|███▎      | 58/178 [00:03<00:06, 18.50it/s]\u001b[A\n",
            " 34%|███▎      | 60/178 [00:03<00:06, 18.50it/s]\u001b[A\n",
            " 35%|███▍      | 62/178 [00:03<00:06, 18.46it/s]\u001b[A\n",
            " 36%|███▌      | 64/178 [00:03<00:06, 18.41it/s]\u001b[A\n",
            " 37%|███▋      | 66/178 [00:03<00:06, 18.37it/s]\u001b[A\n",
            " 38%|███▊      | 68/178 [00:03<00:05, 18.41it/s]\u001b[A\n",
            " 39%|███▉      | 70/178 [00:03<00:05, 18.42it/s]\u001b[A\n",
            " 40%|████      | 72/178 [00:03<00:05, 18.42it/s]\u001b[A\n",
            " 42%|████▏     | 74/178 [00:04<00:05, 18.44it/s]\u001b[A\n",
            " 43%|████▎     | 76/178 [00:04<00:05, 18.34it/s]\u001b[A\n",
            " 44%|████▍     | 78/178 [00:04<00:05, 18.40it/s]\u001b[A\n",
            " 45%|████▍     | 80/178 [00:04<00:05, 18.44it/s]\u001b[A\n",
            " 46%|████▌     | 82/178 [00:04<00:05, 18.38it/s]\u001b[A\n",
            " 47%|████▋     | 84/178 [00:04<00:05, 18.23it/s]\u001b[A\n",
            " 48%|████▊     | 86/178 [00:04<00:05, 18.29it/s]\u001b[A\n",
            " 49%|████▉     | 88/178 [00:04<00:04, 18.34it/s]\u001b[A\n",
            " 51%|█████     | 90/178 [00:04<00:04, 18.41it/s]\u001b[A\n",
            " 52%|█████▏    | 92/178 [00:04<00:04, 18.39it/s]\u001b[A\n",
            " 53%|█████▎    | 94/178 [00:05<00:04, 18.44it/s]\u001b[A\n",
            " 54%|█████▍    | 96/178 [00:05<00:04, 18.54it/s]\u001b[A\n",
            " 55%|█████▌    | 98/178 [00:05<00:04, 18.47it/s]\u001b[A\n",
            " 56%|█████▌    | 100/178 [00:05<00:04, 18.56it/s]\u001b[A\n",
            " 57%|█████▋    | 102/178 [00:05<00:04, 18.58it/s]\u001b[A\n",
            " 58%|█████▊    | 104/178 [00:05<00:03, 18.53it/s]\u001b[A\n",
            " 60%|█████▉    | 106/178 [00:05<00:03, 18.45it/s]\u001b[A\n",
            " 61%|██████    | 108/178 [00:05<00:03, 18.43it/s]\u001b[A\n",
            " 62%|██████▏   | 110/178 [00:05<00:03, 18.23it/s]\u001b[A\n",
            " 63%|██████▎   | 112/178 [00:06<00:03, 18.38it/s]\u001b[A\n",
            " 64%|██████▍   | 114/178 [00:06<00:03, 18.39it/s]\u001b[A\n",
            " 65%|██████▌   | 116/178 [00:06<00:03, 18.37it/s]\u001b[A\n",
            " 66%|██████▋   | 118/178 [00:06<00:03, 18.39it/s]\u001b[A\n",
            " 67%|██████▋   | 120/178 [00:06<00:03, 18.39it/s]\u001b[A\n",
            " 69%|██████▊   | 122/178 [00:06<00:03, 18.42it/s]\u001b[A\n",
            " 70%|██████▉   | 124/178 [00:06<00:02, 18.40it/s]\u001b[A\n",
            " 71%|███████   | 126/178 [00:06<00:02, 18.44it/s]\u001b[A\n",
            " 72%|███████▏  | 128/178 [00:06<00:02, 18.41it/s]\u001b[A\n",
            " 73%|███████▎  | 130/178 [00:07<00:02, 18.36it/s]\u001b[A\n",
            " 74%|███████▍  | 132/178 [00:07<00:02, 18.32it/s]\u001b[A\n",
            " 75%|███████▌  | 134/178 [00:07<00:02, 18.41it/s]\u001b[A\n",
            " 76%|███████▋  | 136/178 [00:07<00:02, 18.48it/s]\u001b[A\n",
            " 78%|███████▊  | 138/178 [00:07<00:02, 18.45it/s]\u001b[A\n",
            " 79%|███████▊  | 140/178 [00:07<00:02, 18.36it/s]\u001b[A\n",
            " 80%|███████▉  | 142/178 [00:07<00:01, 18.35it/s]\u001b[A\n",
            " 81%|████████  | 144/178 [00:07<00:01, 18.36it/s]\u001b[A\n",
            " 82%|████████▏ | 146/178 [00:07<00:01, 18.28it/s]\u001b[A\n",
            " 83%|████████▎ | 148/178 [00:08<00:01, 18.27it/s]\u001b[A\n",
            " 84%|████████▍ | 150/178 [00:08<00:01, 18.30it/s]\u001b[A\n",
            " 85%|████████▌ | 152/178 [00:08<00:01, 18.42it/s]\u001b[A\n",
            " 87%|████████▋ | 154/178 [00:08<00:01, 18.39it/s]\u001b[A\n",
            " 88%|████████▊ | 156/178 [00:08<00:01, 18.41it/s]\u001b[A\n",
            " 89%|████████▉ | 158/178 [00:08<00:01, 18.36it/s]\u001b[A\n",
            " 90%|████████▉ | 160/178 [00:08<00:00, 18.44it/s]\u001b[A\n",
            " 91%|█████████ | 162/178 [00:08<00:00, 18.53it/s]\u001b[A\n",
            " 92%|█████████▏| 164/178 [00:08<00:00, 18.47it/s]\u001b[A\n",
            " 93%|█████████▎| 166/178 [00:09<00:00, 18.50it/s]\u001b[A\n",
            " 94%|█████████▍| 168/178 [00:09<00:00, 18.42it/s]\u001b[A\n",
            " 96%|█████████▌| 170/178 [00:09<00:00, 18.46it/s]\u001b[A\n",
            " 97%|█████████▋| 172/178 [00:09<00:00, 18.46it/s]\u001b[A\n",
            " 98%|█████████▊| 174/178 [00:09<00:00, 18.45it/s]\u001b[A\n",
            "100%|██████████| 178/178 [00:09<00:00, 18.46it/s]\n",
            "100%|█████████▉| 1421/1422 [05:57<00:03,  3.10s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|██████████| 1422/1422 [05:57<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.3184 Val acc: 0.9043 Val f1: 0.9043 AUC: 0.4383\n",
            "\n",
            "\n",
            "Epoch 2 from 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 202/1422 [00:40<04:04,  5.00it/s]\n",
            "  0%|          | 0/178 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 2/178 [00:00<00:09, 18.31it/s]\u001b[A\n",
            "  2%|▏         | 4/178 [00:00<00:09, 18.11it/s]\u001b[A\n",
            "  3%|▎         | 6/178 [00:00<00:09, 18.26it/s]\u001b[A\n",
            "  4%|▍         | 8/178 [00:00<00:09, 18.38it/s]\u001b[A\n",
            "  6%|▌         | 10/178 [00:00<00:09, 18.46it/s]\u001b[A\n",
            "  7%|▋         | 12/178 [00:00<00:09, 18.32it/s]\u001b[A\n",
            "  8%|▊         | 14/178 [00:00<00:08, 18.26it/s]\u001b[A\n",
            "  9%|▉         | 16/178 [00:00<00:08, 18.23it/s]\u001b[A\n",
            " 10%|█         | 18/178 [00:00<00:08, 18.26it/s]\u001b[A\n",
            " 11%|█         | 20/178 [00:01<00:08, 18.34it/s]\u001b[A\n",
            " 12%|█▏        | 22/178 [00:01<00:08, 18.42it/s]\u001b[A\n",
            " 13%|█▎        | 24/178 [00:01<00:08, 18.23it/s]\u001b[A\n",
            " 15%|█▍        | 26/178 [00:01<00:08, 18.25it/s]\u001b[A\n",
            " 16%|█▌        | 28/178 [00:01<00:08, 18.22it/s]\u001b[A\n",
            " 17%|█▋        | 30/178 [00:01<00:08, 18.34it/s]\u001b[A\n",
            " 18%|█▊        | 32/178 [00:01<00:07, 18.30it/s]\u001b[A\n",
            " 19%|█▉        | 34/178 [00:01<00:07, 18.31it/s]\u001b[A\n",
            " 20%|██        | 36/178 [00:01<00:07, 18.38it/s]\u001b[A\n",
            " 21%|██▏       | 38/178 [00:02<00:07, 18.44it/s]\u001b[A\n",
            " 22%|██▏       | 40/178 [00:02<00:07, 18.42it/s]\u001b[A\n",
            " 24%|██▎       | 42/178 [00:02<00:07, 18.48it/s]\u001b[A\n",
            " 25%|██▍       | 44/178 [00:02<00:07, 18.52it/s]\u001b[A\n",
            " 26%|██▌       | 46/178 [00:02<00:07, 18.47it/s]\u001b[A\n",
            " 27%|██▋       | 48/178 [00:02<00:07, 18.52it/s]\u001b[A\n",
            " 28%|██▊       | 50/178 [00:02<00:06, 18.51it/s]\u001b[A\n",
            " 29%|██▉       | 52/178 [00:02<00:06, 18.56it/s]\u001b[A\n",
            " 30%|███       | 54/178 [00:02<00:06, 18.60it/s]\u001b[A\n",
            " 31%|███▏      | 56/178 [00:03<00:06, 18.51it/s]\u001b[A\n",
            " 33%|███▎      | 58/178 [00:03<00:06, 18.52it/s]\u001b[A\n",
            " 34%|███▎      | 60/178 [00:03<00:06, 18.42it/s]\u001b[A\n",
            " 35%|███▍      | 62/178 [00:03<00:06, 18.43it/s]\u001b[A\n",
            " 36%|███▌      | 64/178 [00:03<00:06, 18.39it/s]\u001b[A\n",
            " 37%|███▋      | 66/178 [00:03<00:06, 18.39it/s]\u001b[A\n",
            " 38%|███▊      | 68/178 [00:03<00:05, 18.44it/s]\u001b[A\n",
            " 39%|███▉      | 70/178 [00:03<00:05, 18.36it/s]\u001b[A\n",
            " 40%|████      | 72/178 [00:03<00:05, 18.47it/s]\u001b[A\n",
            " 42%|████▏     | 74/178 [00:04<00:05, 18.52it/s]\u001b[A\n",
            " 43%|████▎     | 76/178 [00:04<00:05, 18.36it/s]\u001b[A\n",
            " 44%|████▍     | 78/178 [00:04<00:05, 18.34it/s]\u001b[A\n",
            " 45%|████▍     | 80/178 [00:04<00:05, 18.43it/s]\u001b[A\n",
            " 46%|████▌     | 82/178 [00:04<00:05, 18.42it/s]\u001b[A\n",
            " 47%|████▋     | 84/178 [00:04<00:05, 18.29it/s]\u001b[A\n",
            " 48%|████▊     | 86/178 [00:04<00:05, 18.29it/s]\u001b[A\n",
            " 49%|████▉     | 88/178 [00:04<00:04, 18.40it/s]\u001b[A\n",
            " 51%|█████     | 90/178 [00:04<00:04, 18.53it/s]\u001b[A\n",
            " 52%|█████▏    | 92/178 [00:05<00:04, 18.47it/s]\u001b[A\n",
            " 53%|█████▎    | 94/178 [00:05<00:04, 18.54it/s]\u001b[A\n",
            " 54%|█████▍    | 96/178 [00:05<00:04, 18.42it/s]\u001b[A\n",
            " 55%|█████▌    | 98/178 [00:05<00:04, 18.34it/s]\u001b[A\n",
            " 56%|█████▌    | 100/178 [00:05<00:04, 18.40it/s]\u001b[A\n",
            " 57%|█████▋    | 102/178 [00:05<00:04, 18.49it/s]\u001b[A\n",
            " 58%|█████▊    | 104/178 [00:05<00:03, 18.51it/s]\u001b[A\n",
            " 60%|█████▉    | 106/178 [00:05<00:03, 18.53it/s]\u001b[A\n",
            " 61%|██████    | 108/178 [00:05<00:03, 18.45it/s]\u001b[A\n",
            " 62%|██████▏   | 110/178 [00:05<00:03, 18.51it/s]\u001b[A\n",
            " 63%|██████▎   | 112/178 [00:06<00:03, 18.49it/s]\u001b[A\n",
            " 64%|██████▍   | 114/178 [00:06<00:03, 18.47it/s]\u001b[A\n",
            " 65%|██████▌   | 116/178 [00:06<00:03, 18.38it/s]\u001b[A\n",
            " 66%|██████▋   | 118/178 [00:06<00:03, 18.40it/s]\u001b[A\n",
            " 67%|██████▋   | 120/178 [00:06<00:03, 18.46it/s]\u001b[A\n",
            " 69%|██████▊   | 122/178 [00:06<00:03, 18.45it/s]\u001b[A\n",
            " 70%|██████▉   | 124/178 [00:06<00:02, 18.44it/s]\u001b[A\n",
            " 71%|███████   | 126/178 [00:06<00:02, 18.49it/s]\u001b[A\n",
            " 72%|███████▏  | 128/178 [00:06<00:02, 18.45it/s]\u001b[A\n",
            " 73%|███████▎  | 130/178 [00:07<00:02, 18.52it/s]\u001b[A\n",
            " 74%|███████▍  | 132/178 [00:07<00:02, 18.38it/s]\u001b[A\n",
            " 75%|███████▌  | 134/178 [00:07<00:02, 18.37it/s]\u001b[A\n",
            " 76%|███████▋  | 136/178 [00:07<00:02, 18.32it/s]\u001b[A\n",
            " 78%|███████▊  | 138/178 [00:07<00:02, 18.33it/s]\u001b[A\n",
            " 79%|███████▊  | 140/178 [00:07<00:02, 18.37it/s]\u001b[A\n",
            " 80%|███████▉  | 142/178 [00:07<00:01, 18.38it/s]\u001b[A\n",
            " 81%|████████  | 144/178 [00:07<00:01, 18.48it/s]\u001b[A\n",
            " 82%|████████▏ | 146/178 [00:07<00:01, 18.47it/s]\u001b[A\n",
            " 83%|████████▎ | 148/178 [00:08<00:01, 18.54it/s]\u001b[A\n",
            " 84%|████████▍ | 150/178 [00:08<00:01, 18.56it/s]\u001b[A\n",
            " 85%|████████▌ | 152/178 [00:08<00:01, 18.52it/s]\u001b[A\n",
            " 87%|████████▋ | 154/178 [00:08<00:01, 18.60it/s]\u001b[A\n",
            " 88%|████████▊ | 156/178 [00:08<00:01, 18.54it/s]\u001b[A\n",
            " 89%|████████▉ | 158/178 [00:08<00:01, 18.46it/s]\u001b[A\n",
            " 90%|████████▉ | 160/178 [00:08<00:00, 18.50it/s]\u001b[A\n",
            " 91%|█████████ | 162/178 [00:08<00:00, 18.55it/s]\u001b[A\n",
            " 92%|█████████▏| 164/178 [00:08<00:00, 18.53it/s]\u001b[A\n",
            " 93%|█████████▎| 166/178 [00:09<00:00, 18.56it/s]\u001b[A\n",
            " 94%|█████████▍| 168/178 [00:09<00:00, 18.59it/s]\u001b[A\n",
            " 96%|█████████▌| 170/178 [00:09<00:00, 18.50it/s]\u001b[A\n",
            " 97%|█████████▋| 172/178 [00:09<00:00, 18.51it/s]\u001b[A\n",
            " 98%|█████████▊| 174/178 [00:09<00:00, 18.48it/s]\u001b[A\n",
            "100%|██████████| 178/178 [00:09<00:00, 18.46it/s]\n",
            " 14%|█▍        | 203/1422 [00:50<1:02:54,  3.10s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            " 14%|█▍        | 204/1422 [00:50<45:10,  2.23s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.3153 Val acc: 0.9043 Val f1: 0.9043 AUC: 0.4384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 405/1422 [01:30<03:23,  5.00it/s]\n",
            "  0%|          | 0/178 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 2/178 [00:00<00:09, 18.91it/s]\u001b[A\n",
            "  2%|▏         | 4/178 [00:00<00:09, 18.82it/s]\u001b[A\n",
            "  3%|▎         | 6/178 [00:00<00:09, 18.70it/s]\u001b[A\n",
            "  4%|▍         | 8/178 [00:00<00:09, 18.59it/s]\u001b[A\n",
            "  6%|▌         | 10/178 [00:00<00:09, 18.50it/s]\u001b[A\n",
            "  7%|▋         | 12/178 [00:00<00:08, 18.54it/s]\u001b[A\n",
            "  8%|▊         | 14/178 [00:00<00:08, 18.48it/s]\u001b[A\n",
            "  9%|▉         | 16/178 [00:00<00:08, 18.33it/s]\u001b[A\n",
            " 10%|█         | 18/178 [00:00<00:08, 18.32it/s]\u001b[A\n",
            " 11%|█         | 20/178 [00:01<00:08, 18.40it/s]\u001b[A\n",
            " 12%|█▏        | 22/178 [00:01<00:08, 18.44it/s]\u001b[A\n",
            " 13%|█▎        | 24/178 [00:01<00:08, 18.44it/s]\u001b[A\n",
            " 15%|█▍        | 26/178 [00:01<00:08, 18.42it/s]\u001b[A\n",
            " 16%|█▌        | 28/178 [00:01<00:08, 18.48it/s]\u001b[A\n",
            " 17%|█▋        | 30/178 [00:01<00:08, 18.36it/s]\u001b[A\n",
            " 18%|█▊        | 32/178 [00:01<00:07, 18.32it/s]\u001b[A\n",
            " 19%|█▉        | 34/178 [00:01<00:07, 18.33it/s]\u001b[A\n",
            " 20%|██        | 36/178 [00:01<00:07, 18.33it/s]\u001b[A\n",
            " 21%|██▏       | 38/178 [00:02<00:07, 18.34it/s]\u001b[A\n",
            " 22%|██▏       | 40/178 [00:02<00:07, 18.36it/s]\u001b[A\n",
            " 24%|██▎       | 42/178 [00:02<00:07, 18.31it/s]\u001b[A\n",
            " 25%|██▍       | 44/178 [00:02<00:07, 18.29it/s]\u001b[A\n",
            " 26%|██▌       | 46/178 [00:02<00:07, 18.27it/s]\u001b[A\n",
            " 27%|██▋       | 48/178 [00:02<00:07, 18.40it/s]\u001b[A\n",
            " 28%|██▊       | 50/178 [00:02<00:06, 18.49it/s]\u001b[A\n",
            " 29%|██▉       | 52/178 [00:02<00:06, 18.47it/s]\u001b[A\n",
            " 30%|███       | 54/178 [00:02<00:06, 18.40it/s]\u001b[A\n",
            " 31%|███▏      | 56/178 [00:03<00:06, 18.34it/s]\u001b[A\n",
            " 33%|███▎      | 58/178 [00:03<00:06, 18.36it/s]\u001b[A\n",
            " 34%|███▎      | 60/178 [00:03<00:06, 18.42it/s]\u001b[A\n",
            " 35%|███▍      | 62/178 [00:03<00:06, 18.43it/s]\u001b[A\n",
            " 36%|███▌      | 64/178 [00:03<00:06, 18.53it/s]\u001b[A\n",
            " 37%|███▋      | 66/178 [00:03<00:06, 18.41it/s]\u001b[A\n",
            " 38%|███▊      | 68/178 [00:03<00:05, 18.37it/s]\u001b[A\n",
            " 39%|███▉      | 70/178 [00:03<00:05, 18.51it/s]\u001b[A\n",
            " 40%|████      | 72/178 [00:03<00:05, 18.48it/s]\u001b[A\n",
            " 42%|████▏     | 74/178 [00:04<00:05, 18.53it/s]\u001b[A\n",
            " 43%|████▎     | 76/178 [00:04<00:05, 18.46it/s]\u001b[A\n",
            " 44%|████▍     | 78/178 [00:04<00:05, 18.40it/s]\u001b[A\n",
            " 45%|████▍     | 80/178 [00:04<00:05, 18.43it/s]\u001b[A\n",
            " 46%|████▌     | 82/178 [00:04<00:05, 18.42it/s]\u001b[A\n",
            " 47%|████▋     | 84/178 [00:04<00:05, 18.46it/s]\u001b[A\n",
            " 48%|████▊     | 86/178 [00:04<00:05, 18.31it/s]\u001b[A\n",
            " 49%|████▉     | 88/178 [00:04<00:04, 18.40it/s]\u001b[A\n",
            " 51%|█████     | 90/178 [00:04<00:04, 18.33it/s]\u001b[A\n",
            " 52%|█████▏    | 92/178 [00:04<00:04, 18.39it/s]\u001b[A\n",
            " 53%|█████▎    | 94/178 [00:05<00:04, 18.43it/s]\u001b[A\n",
            " 54%|█████▍    | 96/178 [00:05<00:04, 18.46it/s]\u001b[A\n",
            " 55%|█████▌    | 98/178 [00:05<00:04, 18.51it/s]\u001b[A\n",
            " 56%|█████▌    | 100/178 [00:05<00:04, 18.41it/s]\u001b[A\n",
            " 57%|█████▋    | 102/178 [00:05<00:04, 18.46it/s]\u001b[A\n",
            " 58%|█████▊    | 104/178 [00:05<00:04, 18.45it/s]\u001b[A\n",
            " 60%|█████▉    | 106/178 [00:05<00:03, 18.42it/s]\u001b[A\n",
            " 61%|██████    | 108/178 [00:05<00:03, 18.45it/s]\u001b[A\n",
            " 62%|██████▏   | 110/178 [00:05<00:03, 18.43it/s]\u001b[A\n",
            " 63%|██████▎   | 112/178 [00:06<00:03, 18.37it/s]\u001b[A\n",
            " 64%|██████▍   | 114/178 [00:06<00:03, 18.51it/s]\u001b[A\n",
            " 65%|██████▌   | 116/178 [00:06<00:03, 18.33it/s]\u001b[A\n",
            " 66%|██████▋   | 118/178 [00:06<00:03, 18.30it/s]\u001b[A\n",
            " 67%|██████▋   | 120/178 [00:06<00:03, 18.36it/s]\u001b[A\n",
            " 69%|██████▊   | 122/178 [00:06<00:03, 18.29it/s]\u001b[A\n",
            " 70%|██████▉   | 124/178 [00:06<00:02, 18.38it/s]\u001b[A\n",
            " 71%|███████   | 126/178 [00:06<00:02, 18.32it/s]\u001b[A\n",
            " 72%|███████▏  | 128/178 [00:06<00:02, 18.41it/s]\u001b[A\n",
            " 73%|███████▎  | 130/178 [00:07<00:02, 18.48it/s]\u001b[A\n",
            " 74%|███████▍  | 132/178 [00:07<00:02, 18.51it/s]\u001b[A\n",
            " 75%|███████▌  | 134/178 [00:07<00:02, 18.50it/s]\u001b[A\n",
            " 76%|███████▋  | 136/178 [00:07<00:02, 18.49it/s]\u001b[A\n",
            " 78%|███████▊  | 138/178 [00:07<00:02, 18.45it/s]\u001b[A\n",
            " 79%|███████▊  | 140/178 [00:07<00:02, 18.49it/s]\u001b[A\n",
            " 80%|███████▉  | 142/178 [00:07<00:01, 18.40it/s]\u001b[A\n",
            " 81%|████████  | 144/178 [00:07<00:01, 18.47it/s]\u001b[A\n",
            " 82%|████████▏ | 146/178 [00:07<00:01, 18.45it/s]\u001b[A\n",
            " 83%|████████▎ | 148/178 [00:08<00:01, 18.48it/s]\u001b[A\n",
            " 84%|████████▍ | 150/178 [00:08<00:01, 18.47it/s]\u001b[A\n",
            " 85%|████████▌ | 152/178 [00:08<00:01, 18.53it/s]\u001b[A\n",
            " 87%|████████▋ | 154/178 [00:08<00:01, 18.48it/s]\u001b[A\n",
            " 88%|████████▊ | 156/178 [00:08<00:01, 18.34it/s]\u001b[A\n",
            " 89%|████████▉ | 158/178 [00:08<00:01, 18.21it/s]\u001b[A\n",
            " 90%|████████▉ | 160/178 [00:08<00:00, 18.26it/s]\u001b[A\n",
            " 91%|█████████ | 162/178 [00:08<00:00, 18.38it/s]\u001b[A\n",
            " 92%|█████████▏| 164/178 [00:08<00:00, 18.36it/s]\u001b[A\n",
            " 93%|█████████▎| 166/178 [00:09<00:00, 18.41it/s]\u001b[A\n",
            " 94%|█████████▍| 168/178 [00:09<00:00, 18.38it/s]\u001b[A\n",
            " 96%|█████████▌| 170/178 [00:09<00:00, 18.43it/s]\u001b[A\n",
            " 97%|█████████▋| 172/178 [00:09<00:00, 18.42it/s]\u001b[A\n",
            " 98%|█████████▊| 174/178 [00:09<00:00, 18.44it/s]\u001b[A\n",
            "100%|██████████| 178/178 [00:09<00:00, 18.45it/s]\n",
            " 29%|██▊       | 406/1422 [01:40<52:29,  3.10s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            " 29%|██▊       | 407/1422 [01:40<37:40,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.3194 Val acc: 0.9043 Val f1: 0.9043 AUC: 0.4484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 608/1422 [02:21<02:42,  5.00it/s]\n",
            "  0%|          | 0/178 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 2/178 [00:00<00:09, 18.53it/s]\u001b[A\n",
            "  2%|▏         | 4/178 [00:00<00:09, 18.51it/s]\u001b[A\n",
            "  3%|▎         | 6/178 [00:00<00:09, 18.47it/s]\u001b[A\n",
            "  4%|▍         | 8/178 [00:00<00:09, 18.46it/s]\u001b[A\n",
            "  6%|▌         | 10/178 [00:00<00:09, 18.43it/s]\u001b[A\n",
            "  7%|▋         | 12/178 [00:00<00:09, 18.41it/s]\u001b[A\n",
            "  8%|▊         | 14/178 [00:00<00:08, 18.46it/s]\u001b[A\n",
            "  9%|▉         | 16/178 [00:00<00:08, 18.46it/s]\u001b[A\n",
            " 10%|█         | 18/178 [00:00<00:08, 18.52it/s]\u001b[A\n",
            " 11%|█         | 20/178 [00:01<00:08, 18.54it/s]\u001b[A\n",
            " 12%|█▏        | 22/178 [00:01<00:08, 18.46it/s]\u001b[A\n",
            " 13%|█▎        | 24/178 [00:01<00:08, 18.44it/s]\u001b[A\n",
            " 15%|█▍        | 26/178 [00:01<00:08, 18.38it/s]\u001b[A\n",
            " 16%|█▌        | 28/178 [00:01<00:08, 18.40it/s]\u001b[A\n",
            " 17%|█▋        | 30/178 [00:01<00:08, 18.41it/s]\u001b[A\n",
            " 18%|█▊        | 32/178 [00:01<00:07, 18.44it/s]\u001b[A\n",
            " 19%|█▉        | 34/178 [00:01<00:07, 18.45it/s]\u001b[A\n",
            " 20%|██        | 36/178 [00:01<00:07, 18.47it/s]\u001b[A\n",
            " 21%|██▏       | 38/178 [00:02<00:07, 18.44it/s]\u001b[A\n",
            " 22%|██▏       | 40/178 [00:02<00:07, 18.36it/s]\u001b[A\n",
            " 24%|██▎       | 42/178 [00:02<00:07, 18.32it/s]\u001b[A\n",
            " 25%|██▍       | 44/178 [00:02<00:07, 18.32it/s]\u001b[A\n",
            " 26%|██▌       | 46/178 [00:02<00:07, 18.31it/s]\u001b[A\n",
            " 27%|██▋       | 48/178 [00:02<00:07, 18.35it/s]\u001b[A\n",
            " 28%|██▊       | 50/178 [00:02<00:06, 18.38it/s]\u001b[A\n",
            " 29%|██▉       | 52/178 [00:02<00:06, 18.32it/s]\u001b[A\n",
            " 30%|███       | 54/178 [00:02<00:06, 18.33it/s]\u001b[A\n",
            " 31%|███▏      | 56/178 [00:03<00:06, 18.21it/s]\u001b[A\n",
            " 33%|███▎      | 58/178 [00:03<00:06, 18.31it/s]\u001b[A\n",
            " 34%|███▎      | 60/178 [00:03<00:06, 18.36it/s]\u001b[A\n",
            " 35%|███▍      | 62/178 [00:03<00:06, 18.42it/s]\u001b[A\n",
            " 36%|███▌      | 64/178 [00:03<00:06, 18.31it/s]\u001b[A\n",
            " 37%|███▋      | 66/178 [00:03<00:06, 18.12it/s]\u001b[A\n",
            " 38%|███▊      | 68/178 [00:03<00:06, 18.21it/s]\u001b[A\n",
            " 39%|███▉      | 70/178 [00:03<00:05, 18.24it/s]\u001b[A\n",
            " 40%|████      | 72/178 [00:03<00:05, 18.23it/s]\u001b[A\n",
            " 42%|████▏     | 74/178 [00:04<00:05, 18.30it/s]\u001b[A\n",
            " 43%|████▎     | 76/178 [00:04<00:05, 18.43it/s]\u001b[A\n",
            " 44%|████▍     | 78/178 [00:04<00:05, 18.47it/s]\u001b[A\n",
            " 45%|████▍     | 80/178 [00:04<00:05, 18.44it/s]\u001b[A\n",
            " 46%|████▌     | 82/178 [00:04<00:05, 18.40it/s]\u001b[A\n",
            " 47%|████▋     | 84/178 [00:04<00:05, 18.37it/s]\u001b[A\n",
            " 48%|████▊     | 86/178 [00:04<00:05, 18.38it/s]\u001b[A\n",
            " 49%|████▉     | 88/178 [00:04<00:04, 18.43it/s]\u001b[A\n",
            " 51%|█████     | 90/178 [00:04<00:04, 18.47it/s]\u001b[A\n",
            " 52%|█████▏    | 92/178 [00:05<00:04, 18.33it/s]\u001b[A\n",
            " 53%|█████▎    | 94/178 [00:05<00:04, 18.29it/s]\u001b[A\n",
            " 54%|█████▍    | 96/178 [00:05<00:04, 18.39it/s]\u001b[A\n",
            " 55%|█████▌    | 98/178 [00:05<00:04, 18.36it/s]\u001b[A\n",
            " 56%|█████▌    | 100/178 [00:05<00:04, 18.40it/s]\u001b[A\n",
            " 57%|█████▋    | 102/178 [00:05<00:04, 18.40it/s]\u001b[A\n",
            " 58%|█████▊    | 104/178 [00:05<00:04, 18.49it/s]\u001b[A\n",
            " 60%|█████▉    | 106/178 [00:05<00:03, 18.52it/s]\u001b[A\n",
            " 61%|██████    | 108/178 [00:05<00:03, 18.49it/s]\u001b[A\n",
            " 62%|██████▏   | 110/178 [00:05<00:03, 18.53it/s]\u001b[A\n",
            " 63%|██████▎   | 112/178 [00:06<00:03, 18.36it/s]\u001b[A\n",
            " 64%|██████▍   | 114/178 [00:06<00:03, 18.41it/s]\u001b[A\n",
            " 65%|██████▌   | 116/178 [00:06<00:03, 18.36it/s]\u001b[A\n",
            " 66%|██████▋   | 118/178 [00:06<00:03, 18.36it/s]\u001b[A\n",
            " 67%|██████▋   | 120/178 [00:06<00:03, 18.43it/s]\u001b[A\n",
            " 69%|██████▊   | 122/178 [00:06<00:03, 18.46it/s]\u001b[A\n",
            " 70%|██████▉   | 124/178 [00:06<00:02, 18.50it/s]\u001b[A\n",
            " 71%|███████   | 126/178 [00:06<00:02, 18.45it/s]\u001b[A\n",
            " 72%|███████▏  | 128/178 [00:06<00:02, 18.49it/s]\u001b[A\n",
            " 73%|███████▎  | 130/178 [00:07<00:02, 18.38it/s]\u001b[A\n",
            " 74%|███████▍  | 132/178 [00:07<00:02, 18.45it/s]\u001b[A\n",
            " 75%|███████▌  | 134/178 [00:07<00:02, 18.52it/s]\u001b[A\n",
            " 76%|███████▋  | 136/178 [00:07<00:02, 18.47it/s]\u001b[A\n",
            " 78%|███████▊  | 138/178 [00:07<00:02, 18.42it/s]\u001b[A\n",
            " 79%|███████▊  | 140/178 [00:07<00:02, 18.52it/s]\u001b[A\n",
            " 80%|███████▉  | 142/178 [00:07<00:01, 18.48it/s]\u001b[A\n",
            " 81%|████████  | 144/178 [00:07<00:01, 18.51it/s]\u001b[A\n",
            " 82%|████████▏ | 146/178 [00:07<00:01, 18.45it/s]\u001b[A\n",
            " 83%|████████▎ | 148/178 [00:08<00:01, 18.43it/s]\u001b[A\n",
            " 84%|████████▍ | 150/178 [00:08<00:01, 18.46it/s]\u001b[A\n",
            " 85%|████████▌ | 152/178 [00:08<00:01, 18.46it/s]\u001b[A\n",
            " 87%|████████▋ | 154/178 [00:08<00:01, 18.51it/s]\u001b[A\n",
            " 88%|████████▊ | 156/178 [00:08<00:01, 18.55it/s]\u001b[A\n",
            " 89%|████████▉ | 158/178 [00:08<00:01, 18.49it/s]\u001b[A\n",
            " 90%|████████▉ | 160/178 [00:08<00:00, 18.55it/s]\u001b[A\n",
            " 91%|█████████ | 162/178 [00:08<00:00, 18.57it/s]\u001b[A\n",
            " 92%|█████████▏| 164/178 [00:08<00:00, 18.51it/s]\u001b[A\n",
            " 93%|█████████▎| 166/178 [00:09<00:00, 18.57it/s]\u001b[A\n",
            " 94%|█████████▍| 168/178 [00:09<00:00, 18.56it/s]\u001b[A\n",
            " 96%|█████████▌| 170/178 [00:09<00:00, 18.63it/s]\u001b[A\n",
            " 97%|█████████▋| 172/178 [00:09<00:00, 18.71it/s]\u001b[A\n",
            " 98%|█████████▊| 174/178 [00:09<00:00, 18.71it/s]\u001b[A\n",
            "100%|██████████| 178/178 [00:09<00:00, 18.46it/s]\n",
            " 43%|████▎     | 609/1422 [02:30<41:57,  3.10s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            " 43%|████▎     | 610/1422 [02:31<30:06,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.3159 Val acc: 0.9043 Val f1: 0.9043 AUC: 0.4458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 649/1422 [02:38<03:09,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c0d2fdbd8f9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# # When using GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def final_prediction(model, dataloader): \n",
        "        preds = None\n",
        "        proba = None\n",
        "        all_ids = None\n",
        "        for batch in tqdm(dataloader):\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                ids = batch['ids'].to(device, dtype = torch.long)\n",
        "                mask = batch['mask'].to(device, dtype = torch.long)\n",
        "                token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
        "                targets = batch['targets']\n",
        "                targets=targets.view(targets.size(0),-1)\n",
        "                labels=targets.to(device, dtype = torch.float32)\n",
        "                outputs = model(ids, mask, token_type_ids)\n",
        "                logits = outputs\n",
        "            if preds is None:\n",
        "                # all_ids = ids.detach().cpu().numpy()\n",
        "                preds = torch.sigmoid(logits).detach().cpu().numpy() > 0.5\n",
        "                proba = torch.sigmoid(logits).detach().cpu().numpy()            \n",
        "            else:  \n",
        "                # all_ids = np.append(all_ids, ids.detach().cpu().numpy(), axis=0)\n",
        "                preds = np.append(preds, torch.sigmoid(logits).detach().cpu().numpy() > 0.5, axis=0)\n",
        "                proba = np.append(proba, torch.sigmoid(logits).detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        result = {\n",
        "            # \"ids\": all_ids,\n",
        "            \"preds\": preds,\n",
        "            \"probs\": proba,\n",
        "        }\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "53d5QnnNJj68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/MyDrive/RajWorkspace/mytestresults/model-auc0.913-loss0.186-acc0.935.pt'\n",
        "bestmodel_final=load_checkpoint(path,model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8yfWcb6DrGJ",
        "outputId": "e924efce-b90b-46f1-af10-791a29578a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from <== /content/drive/MyDrive/RajWorkspace/mytestresults/model-auc0.913-loss0.186-acc0.935.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_params = {'batch_size': 1,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "eval_loader = DataLoader(testing_set, **eval_params)"
      ],
      "metadata": {
        "id": "3ARAHFMT51f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_final=final_prediction(bestmodel_final, eval_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhjd_d2-J8L8",
        "outputId": "ab2496eb-ffb6-4cce-895c-59d39ebe681a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2841 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|██████████| 2841/2841 [00:46<00:00, 61.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resf=pd.DataFrame(results_final['preds'])\n",
        "resf.columns=['label']\n",
        "resf.loc[resf['label']== True, \"label\"] = 1\n",
        "resf.loc[resf['label']== False, \"label\"] = 0\n",
        "resf['label']=resf['label'].astype(str).astype(int)"
      ],
      "metadata": {
        "id": "YPaU9egjKC6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test.label, resf.label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5wnochqKMU9",
        "outputId": "6d057f1e-23fc-420b-c553-977b53658a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96      2569\n",
            "           1       0.70      0.57      0.63       272\n",
            "\n",
            "    accuracy                           0.94      2841\n",
            "   macro avg       0.83      0.77      0.80      2841\n",
            "weighted avg       0.93      0.94      0.93      2841\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resf.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3bKsHatKQmz",
        "outputId": "e8899cb9-9d9f-46bb-8849-e4016bb261b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0        2619\n",
              "1         222\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GYLZrk2_PzVq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}